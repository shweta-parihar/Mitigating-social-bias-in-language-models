{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xrLZunJch-w2"},"outputs":[],"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"SQNPAYfbbsZU"},"source":["# ***Constant Beta value in all attention heads***"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"SdC2tfTLidIw","executionInfo":{"status":"ok","timestamp":1713981376685,"user_tz":300,"elapsed":8669,"user":{"displayName":"Shweta","userId":"05242245977803223502"}}},"outputs":[],"source":["import torch\n","from transformers import BertTokenizer, BertForMaskedLM\n","\n","import logging\n","\n","# Disable CUDNN benchmark mode\n","torch.backends.cudnn.benchmark = False\n","\n","# Set logging level to suppress warnings\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","\n","def modify_attention_heads(model, scalar_values):\n","    model_dict = model.state_dict()\n","\n","    # Identify the keys corresponding to the attention heads\n","    attention_keys = [key for key in model_dict.keys() if 'attention.self.query.weight' in key]\n","\n","    # Modify each attention head with scalar values\n","    for key, scalar in zip(attention_keys, scalar_values):\n","        # Extract the layer number\n","        layer_num = key.split('bert.encoder.layer.')[1].split('.')[0]\n","\n","        # Update query, key, and value weights\n","        model_dict[f'bert.encoder.layer.{layer_num}.attention.self.query.weight'] *= scalar\n","        # model_dict[f'bert.encoder.layer.{layer_num}.attention.self.key.weight'] *= scalar\n","        # model_dict[f'bert.encoder.layer.{layer_num}.attention.self.value.weight'] *= scalar\n","\n","    model.load_state_dict(model_dict)\n","    return model\n","\n","\n","# Function to get masked token probabilities\n","def get_masked_token_probabilities(sentence, mask_words_list):\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","\n","    # Modify attention heads with scalar values\n","    model = modify_attention_heads(model, scalar_values)\n","\n","    inputs = tokenizer(sentence, return_tensors=\"pt\")\n","    mask_token_index = torch.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)[0]\n","\n","    # Ensure only one mask token in the sentence\n","    if len(mask_token_index) != 1:\n","        raise ValueError(\"Please provide a sentence with exactly one [MASK] token.\")\n","\n","    mask_token_index = mask_token_index.item()\n","\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Get logits for mask token\n","    logits = outputs.logits\n","    mask_token_logits = logits[0, mask_token_index, :]\n","\n","    # Calculate probabilities\n","    probabilities = torch.softmax(mask_token_logits, dim=0)\n","\n","    # Convert token ids to words\n","    mask_token_id = torch.argmax(probabilities).item()\n","    mask_word = tokenizer.convert_ids_to_tokens(mask_token_id)\n","\n","    # Filter probabilities for mask_words_list\n","    word_probabilities = {word: probabilities[tokenizer.convert_tokens_to_ids(word)].item() for word in mask_words_list}\n","\n","    return word_probabilities"]},{"cell_type":"markdown","metadata":{"id":"fOPOw00xcJag"},"source":["# ***Winogender***"]},{"cell_type":"markdown","metadata":{"id":"_ALkDHl-AZh0"},"source":[]},{"cell_type":"markdown","metadata":{"id":"ukgVwf_ToiWg"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305409,"status":"ok","timestamp":1713603367443,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"zvuJLW-BrQ5H","outputId":"d4bde33b-5c7c-4d47-e63d-c16c29d4a79c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.008869439363479614\n","she: 0.006484703626483679\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = 0.04 in all attention heads --> Winogender - Average gender bias in bert:  0.18\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.04 in all attention heads --> Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":303737,"status":"ok","timestamp":1713603671176,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"DZw3R_xslD39","outputId":"d49b465a-c832-4d38-b04b-d5e51084f51e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.012616428546607494\n","she: 0.008298131637275219\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = 0.1 in all attention heads --> Winogender - Average gender bias in bert:  0.2\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.1 in all attention heads --> Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":291216,"status":"ok","timestamp":1713603962384,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"X3a-lvHEWfcy","outputId":"63c7d078-b7a5-4175-d649-bf55f784aae7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.4920428991317749\n","she: 0.0015146428486332297\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = 4 in all attention heads --> Winogender - Average gender bias in bert:  0.63\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 4 in all attention heads --> Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288548,"status":"ok","timestamp":1713604250925,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"l4cboww_rAtp","outputId":"7efcb6bc-262a-4d2d-f714-88d633a002e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.00780993839725852\n","she: 0.005908448249101639\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = 0.01 in all attention heads --> Winogender - Average gender bias in bert:  0.17\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.01 in all attention heads --> Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnOTsVGTs6s4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdjrBtqIcOYU"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"UgXDsRPVcPrS"},"source":["# ***Winobias***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOxrkSRYcOVv"},"outputs":[],"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"CdP-t4PzcWF6"},"source":[]},{"cell_type":"markdown","metadata":{"id":"DkVSqW6VcWF6"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"elapsed":207,"status":"error","timestamp":1713885982717,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"AWtka6DCcWF7","outputId":"02334121-93a0-4c30-b85e-4b1549d3140d"},"outputs":[{"ename":"NameError","evalue":"name 'get_masked_token_probabilities' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d6d00f3d9f69>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Get the probabilities of specified words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mword_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_masked_token_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_words_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Probabilities of specified words:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'get_masked_token_probabilities' is not defined"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winobias = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winobias_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winobias[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winobias = df_winobias.apply(apply_bert, axis=1)\n","\n","average_bias_winobias = round(df_winobias['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.04 in all attention heads --> Winobias- Average gender bias in bert: ', average_bias_winobias)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MakMWLSlBYNS"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winobias = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winobias_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winobias[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winobias = df_winobias.apply(apply_bert, axis=1)\n","\n","average_bias_winobias = round(df_winobias['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.4 in all attention heads --> Winobias- Average gender bias in bert: ', average_bias_winobias)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzyP_An5BYU_"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winobias = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winobias_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winobias[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winobias = df_winobias.apply(apply_bert, axis=1)\n","\n","average_bias_winobias = round(df_winobias['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 4 in all attention heads --> Winobias- Average gender bias in bert: ', average_bias_winobias)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGxqwxgXdj_j"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winobias = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winobias_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winobias[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winobias = df_winobias.apply(apply_bert, axis=1)\n","\n","average_bias_winobias = round(df_winobias['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.01 in all attention heads --> Winobias- Average gender bias in bert: ', average_bias_winobias)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DrIJ2CRds_z"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winobias = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winobias_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winobias[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winobias = df_winobias.apply(apply_bert, axis=1)\n","\n","average_bias_winobias = round(df_winobias['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.1 in all attention heads --> Winobias- Average gender bias in bert: ', average_bias_winobias)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_v-lGMGd23h"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winobias = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winobias_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winobias[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winobias = df_winobias.apply(apply_bert, axis=1)\n","\n","average_bias_winobias = round(df_winobias['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 1 in all attention heads --> Winobias- Average gender bias in bert: ', average_bias_winobias)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivvrgAiSeinF"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BPEYxn5fejP2"},"source":["# ***Stereoset***"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3c5c4526a36c4946898e5d1d0c114ff0","cbb658ce9c7b490abe1a05c507aad8c8","e0b9694fe2064b1d9875bb4c3ed663f5","3a840bd0abd244e58f09cc07b96fdcfd","617bb296b2424d47a0ea04f9324ba407","752999873847421c9e4db630bab1827d","a7bf06895e8f48f2a6fa85fd93688640","10003730393a4804a793653ac82bda3e","529b1676ef994f51a7c5060bf5194a55","f02038a912a247219e9789a4a89e1314","00e197bd2f2a47e58cd958c8184d9205","77156009fc3342c5bf4acc43b20f9356","8081fe6be0e244abb722b3d3a5e43831","7d902539f111494ebe817935650f0bce","b053cadcf6dd41039ac86324ccd29d86","3b9ae6c5d9704c6aa74dda821b5461f7","ebc83931de3247fea8e70f591dc55167","d4886e5dbd834e3faa34235cfc81ac0f","1c1c4a1fb4724ece8d2c9525cdf1053a","9495d6620c2e474fb98d42e4e8aaebfe","bda0febee4674b16880acde0ad7935b5","114bae8eefeb42c289963d5954a38641","5dd1b51f898e4ebebd981249b5d67e53","0476e8fb33014ef6a327a3c8167cba7d","14958c7b84a445cc8beedd2a9374f7ea","eaaffa88b4b34821be6372bc3d1beb79","677192ca7ed7491e8e143d25145aa2c7","5bd047741ce8402ca13666c65c101e55","bbd72edfd1bd4d01a158b46a75adf17e","60dea3cfedc34a7889c3a5e70456935b","be07331c07004d0197c75703f9a5d94e","822ddadb6a2b458b907f574ac9ebf181","ea3f8a64253a4bdf8f5f40f26f351404","b5fdc2aff4264e1c849c45ca70a922e9","2dfe41791e464eebab8b628318511fe6","2172ce02f14a437f8e2e69fbb89f3802","38b2adb5ab4842abaf8a23d5fab0b0c2","03d18487da9a45cb9da6c3e5da29c82d","c6c7c1dd45eb4bf79134a7657d009046","c0895aa8acb949b3a4b28aba49429007","536dca3fb93c4fc2b49a1aec5989bea0","37d95145fa4746f1a770a37f6dee90cb","2983367c06c24b40bd3b8205878c02d7","b6112557e483492da42ba2eba4bff811","a11f133bfe19446a924e2d835fd9c9ab","d853c79e8d4649bfbf5a888f6a8f9968","f3da463594cb4d4eaccbc0dc6b8cd3ba","785f9b9d35e149d69d9d72bb01c8bdba","178947d93e9145c8a16493818e9949cd","350d1971596341229c34af4d132025ae","ab90d937469b41fbb6149996262a7f84","6df6001ccc814a3685c60868697d1a36","f60febbdc302457f8ef371b9c8a166d8","d3e56e28296947b6b565abead3e012b9","9f585d6d889044b5ae7729a7d35b6cf2"]},"executionInfo":{"elapsed":1176119,"status":"ok","timestamp":1713887193269,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"rgP38rgVelEy","outputId":"761db9b8-75c4-4fac-b02d-20c878e56ca8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c5c4526a36c4946898e5d1d0c114ff0","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77156009fc3342c5bf4acc43b20f9356","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5dd1b51f898e4ebebd981249b5d67e53","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b5fdc2aff4264e1c849c45ca70a922e9","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a11f133bfe19446a924e2d835fd9c9ab","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.008869439363479614\n","she: 0.006484703626483679\n","Mounted at /content/drive\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-7-632cdf700df3>:50: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[selected_columns] = df_stereoset[selected_columns].applymap(lowercase_except_mask)\n","<ipython-input-7-632cdf700df3>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-7-632cdf700df3>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-7-632cdf700df3>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-7-632cdf700df3>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-7-632cdf700df3>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------\n","Beta = 0.04 in all attention heads --> Stereoset - Average bias in bert: 0.41\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_stereoset = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Stereoset_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_stereoset.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_stereoset = df_stereoset[mask]\n","df_stereoset\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_stereoset[selected_columns] = df_stereoset[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_stereoset[col_name] = None\n","df_stereoset\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_stereoset = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_stereoset.iterrows())\n","df_stereoset = pd.DataFrame(df_stereoset)\n","df_stereoset\n","\n","\n","average_bias_stereoset = round(df_stereoset['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.04 in all attention heads --> Stereoset - Average bias in bert:', average_bias_stereoset)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1052898,"status":"ok","timestamp":1713888246140,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"3nmDl5U-g5_8","outputId":"a0de42a1-ebf2-4270-e71e-998b5a96735d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.5928203463554382\n","she: 0.08490896224975586\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = 0.4 in all attention heads --> Stereoset - Average bias in bert: 0.56\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_stereoset = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Stereoset_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_stereoset.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_stereoset = df_stereoset[mask]\n","df_stereoset\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_stereoset[selected_columns] = df_stereoset[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_stereoset[col_name] = None\n","df_stereoset\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_stereoset = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_stereoset.iterrows())\n","df_stereoset = pd.DataFrame(df_stereoset)\n","df_stereoset\n","\n","\n","average_bias_stereoset = round(df_stereoset['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.4 in all attention heads --> Stereoset - Average bias in bert:', average_bias_stereoset)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1062368,"status":"ok","timestamp":1713889308482,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"8xDdmcCsg59l","outputId":"5492394a-a071-414a-a3e7-d9012065c538"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.4920428991317749\n","she: 0.0015146428486332297\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-9-154123959ec9>:50: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[selected_columns] = df_stereoset[selected_columns].applymap(lowercase_except_mask)\n","<ipython-input-9-154123959ec9>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-9-154123959ec9>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-9-154123959ec9>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-9-154123959ec9>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-9-154123959ec9>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------\n","Beta = 4 in all attention heads --> Stereoset - Average bias in bert: 0.65\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_stereoset = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Stereoset_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_stereoset.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_stereoset = df_stereoset[mask]\n","df_stereoset\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_stereoset[selected_columns] = df_stereoset[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_stereoset[col_name] = None\n","df_stereoset\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_stereoset = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_stereoset.iterrows())\n","df_stereoset = pd.DataFrame(df_stereoset)\n","df_stereoset\n","\n","\n","average_bias_stereoset = round(df_stereoset['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 4 in all attention heads --> Stereoset - Average bias in bert:', average_bias_stereoset)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"rxBYsYTqg56F","outputId":"2271f4ba-f529-40a9-9621-58d68bed52f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.00780993839725852\n","she: 0.005908448249101639\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-10-489216586d20>:50: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[selected_columns] = df_stereoset[selected_columns].applymap(lowercase_except_mask)\n","<ipython-input-10-489216586d20>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-10-489216586d20>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-10-489216586d20>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-10-489216586d20>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","<ipython-input-10-489216586d20>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_stereoset[col_name] = None\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------\n","Beta = 0.01 in all attention heads --> Stereoset - Average bias in bert: 0.41\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_stereoset = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Stereoset_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_stereoset.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_stereoset = df_stereoset[mask]\n","df_stereoset\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_stereoset[selected_columns] = df_stereoset[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_stereoset[col_name] = None\n","df_stereoset\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_stereoset = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_stereoset.iterrows())\n","df_stereoset = pd.DataFrame(df_stereoset)\n","df_stereoset\n","\n","\n","average_bias_stereoset = round(df_stereoset['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.01 in all attention heads --> Stereoset - Average bias in bert:', average_bias_stereoset)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TFr2U208g54P","outputId":"3148f632-9819-4852-bdcd-5a52d615db83"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.012616428546607494\n","she: 0.008298131637275219\n"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_stereoset = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Stereoset_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_stereoset.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_stereoset = df_stereoset[mask]\n","df_stereoset\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_stereoset[selected_columns] = df_stereoset[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_stereoset[col_name] = None\n","df_stereoset\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_stereoset = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_stereoset.iterrows())\n","df_stereoset = pd.DataFrame(df_stereoset)\n","df_stereoset\n","\n","\n","average_bias_stereoset = round(df_stereoset['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.1 in all attention heads --> Stereoset - Average bias in bert:', average_bias_stereoset)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qK1zbL0Eg51N"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_stereoset = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Stereoset_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_stereoset.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_stereoset = df_stereoset[mask]\n","df_stereoset\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_stereoset[selected_columns] = df_stereoset[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_stereoset[col_name] = None\n","df_stereoset\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_stereoset = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_stereoset.iterrows())\n","df_stereoset = pd.DataFrame(df_stereoset)\n","df_stereoset\n","\n","\n","average_bias_stereoset = round(df_stereoset['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 1 in all attention heads --> Stereoset - Average bias in bert:', average_bias_stereoset)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"markdown","metadata":{"id":"V9YLAbb7h4t3"},"source":["# ***Crowspairs***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TA1tjp8gh71I"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_crowspairs = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Crowspairs_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_crowspairs.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_crowspairs = df_crowspairs[mask]\n","df_crowspairs\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_crowspairs[selected_columns] = df_crowspairs[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_crowspairs[col_name] = None\n","df_crowspairs\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_crowspairs = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_crowspairs.iterrows())\n","df_crowspairs = pd.DataFrame(df_crowspairs)\n","df_crowspairs\n","\n","\n","average_bias_crowspairs = round(df_crowspairs['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.04 in all attention heads --> Crowspairs - Average bias in bert:', average_bias_crowspairs)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkP1aBvTjNWh"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_crowspairs = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Crowspairs_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_crowspairs.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_crowspairs = df_crowspairs[mask]\n","df_crowspairs\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_crowspairs[selected_columns] = df_crowspairs[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_crowspairs[col_name] = None\n","df_crowspairs\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_crowspairs = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_crowspairs.iterrows())\n","df_crowspairs = pd.DataFrame(df_crowspairs)\n","df_crowspairs\n","\n","\n","average_bias_crowspairs = round(df_crowspairs['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.4 in all attention heads --> Crowspairs - Average bias in bert:', average_bias_crowspairs)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mbZDjp_nivMS"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_crowspairs = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Crowspairs_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_crowspairs.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_crowspairs = df_crowspairs[mask]\n","df_crowspairs\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_crowspairs[selected_columns] = df_crowspairs[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_crowspairs[col_name] = None\n","df_crowspairs\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_crowspairs = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_crowspairs.iterrows())\n","df_crowspairs = pd.DataFrame(df_crowspairs)\n","df_crowspairs\n","\n","\n","average_bias_crowspairs = round(df_crowspairs['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 4 in all attention heads --> Crowspairs - Average bias in bert:', average_bias_crowspairs)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y07HCejIo_ym"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_crowspairs = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Crowspairs_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_crowspairs.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_crowspairs = df_crowspairs[mask]\n","df_crowspairs\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_crowspairs[selected_columns] = df_crowspairs[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_crowspairs[col_name] = None\n","df_crowspairs\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_crowspairs = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_crowspairs.iterrows())\n","df_crowspairs = pd.DataFrame(df_crowspairs)\n","df_crowspairs\n","\n","\n","average_bias_crowspairs = round(df_crowspairs['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.01 in all attention heads --> Crowspairs - Average bias in bert:', average_bias_crowspairs)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Buj0YTxBpSBG"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_crowspairs = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Crowspairs_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_crowspairs.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_crowspairs = df_crowspairs[mask]\n","df_crowspairs\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_crowspairs[selected_columns] = df_crowspairs[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_crowspairs[col_name] = None\n","df_crowspairs\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_crowspairs = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_crowspairs.iterrows())\n","df_crowspairs = pd.DataFrame(df_crowspairs)\n","df_crowspairs\n","\n","\n","average_bias_crowspairs = round(df_crowspairs['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 0.1 in all attention heads --> Crowspairs - Average bias in bert:', average_bias_crowspairs)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIbBhcRNpv6K"},"outputs":[],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_crowspairs = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Crowspairs_dataset.csv\")\n","\n","\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to check if word is in BERT vocabulary\n","def word_in_vocab(word):\n","    return word in tokenizer.vocab\n","\n","# Filter rows where words in stereotype or anti_stereotype column are present in BERT vocabulary\n","mask = df_crowspairs.apply(lambda row: word_in_vocab(row['stereotype']) and word_in_vocab(row['anti_stereotype']), axis=1)\n","df_crowspairs = df_crowspairs[mask]\n","df_crowspairs\n","\n","\n","import re\n","# Define a function to lowercase everything except [MASK]\n","def lowercase_except_mask(text):\n","    # Use regex to find [MASK] and preserve it while converting the rest to lowercase\n","    return ' '.join('[MASK]' if '[MASK]' in word else word.lower() for word in text.split())\n","\n","# Apply the function to selected columns\n","selected_columns = ['masked_sentence', 'stereotype','anti_stereotype']\n","df_crowspairs[selected_columns] = df_crowspairs[selected_columns].applymap(lowercase_except_mask)\n","\n","new_cols = ['stereo_prob_abs','antistereo_prob_abs', 'stereo_prob_percent','antistereo_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_crowspairs[col_name] = None\n","df_crowspairs\n","\n","\n","from joblib import Parallel, delayed\n","\n","def apply_bert_parallel(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['stereotype'], row['anti_stereotype']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  for word in mask_words_list:\n","      if word not in word_probabilities:\n","          print('Word not found')\n","          word_probabilities[word] = 0.01\n","\n","  row['stereo_prob_abs'], row['antistereo_prob_abs'] = round(word_probabilities[row['stereotype']],2) , round(word_probabilities[row['anti_stereotype']],2)\n","  row['stereo_prob_percent'] = round(word_probabilities[row['stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['antistereo_prob_percent'] = round(word_probabilities[row['anti_stereotype']] / (word_probabilities[row['stereotype']] + word_probabilities[row['anti_stereotype']]),2)\n","  row['bias_percent'] = round(abs(row['stereo_prob_percent'] - row['antistereo_prob_percent']),2)\n","  return row\n","\n","# Define the number of parallel jobs\n","num_cores = 10  # Adjust according to your machine's specifications\n","\n","# Apply the function to each row in parallel\n","df_crowspairs = Parallel(n_jobs=num_cores)(delayed(apply_bert_parallel)(row) for _, row in df_crowspairs.iterrows())\n","df_crowspairs = pd.DataFrame(df_crowspairs)\n","df_crowspairs\n","\n","\n","average_bias_crowspairs = round(df_crowspairs['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = 1 in all attention heads --> Crowspairs - Average bias in bert:', average_bias_crowspairs)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"markdown","metadata":{"id":"xwUWBra-PMTf"},"source":["# ***Random Search Cases:***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rci1O20cFzDU"},"outputs":[],"source":["import torch\n","from transformers import BertTokenizer, BertForMaskedLM\n","\n","import logging\n","\n","# Disable CUDNN benchmark mode\n","torch.backends.cudnn.benchmark = False\n","\n","# Set logging level to suppress warnings\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","\n","def modify_attention_heads(model, scalar_values):\n","    model_dict = model.state_dict()\n","\n","    # Identify the keys corresponding to the attention heads\n","    attention_keys = [key for key in model_dict.keys() if 'attention.self.query.weight' in key]\n","\n","    # Modify each attention head with scalar values\n","    for key, scalar in zip(attention_keys, scalar_values):\n","        # Extract the layer number\n","        layer_num = key.split('bert.encoder.layer.')[1].split('.')[0]\n","\n","        # Update query, key, and value weights\n","        model_dict[f'bert.encoder.layer.{layer_num}.attention.self.query.weight'] *= scalar\n","        # model_dict[f'bert.encoder.layer.{layer_num}.attention.self.key.weight'] *= scalar\n","        # model_dict[f'bert.encoder.layer.{layer_num}.attention.self.value.weight'] *= scalar\n","\n","    model.load_state_dict(model_dict)\n","    return model\n","\n","\n","# Function to get masked token probabilities\n","def get_masked_token_probabilities(sentence, mask_words_list):\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","\n","    # Modify attention heads with scalar values\n","    model = modify_attention_heads(model, scalar_values)\n","\n","    inputs = tokenizer(sentence, return_tensors=\"pt\")\n","    mask_token_index = torch.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)[0]\n","\n","    # Ensure only one mask token in the sentence\n","    if len(mask_token_index) != 1:\n","        raise ValueError(\"Please provide a sentence with exactly one [MASK] token.\")\n","\n","    mask_token_index = mask_token_index.item()\n","\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Get logits for mask token\n","    logits = outputs.logits\n","    mask_token_logits = logits[0, mask_token_index, :]\n","\n","    # Calculate probabilities\n","    probabilities = torch.softmax(mask_token_logits, dim=0)\n","\n","    # Convert token ids to words\n","    mask_token_id = torch.argmax(probabilities).item()\n","    mask_word = tokenizer.convert_ids_to_tokens(mask_token_id)\n","\n","    # Filter probabilities for mask_words_list\n","    word_probabilities = {word: probabilities[tokenizer.convert_tokens_to_ids(word)].item() for word in mask_words_list}\n","\n","    return word_probabilities"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51446,"status":"ok","timestamp":1713613015186,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"HgeK1FEaIduP","outputId":"48993f00-f23b-4f61-960b-cffc73d7c7a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.1, 0.01, 0.1, 0.1, 1, 0.01, 0.01, 0.01, 10, 0.01, 1, 0.1] \t 0.0072\n","[0.01, 0.1, 0.01, 0.01, 0.01, 0.1, 0.1, 1, 0.01, 0.1, 10, 1] \t 0.0035\n","[0.1, 0.01, 0.1, 0.1, 0.1, 0.1, 10, 1, 1, 10, 10, 1] \t 0.001\n"]}],"source":["possible_values = []\n","\n","def run():\n","  # Example sentence\n","  sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","  # List of words to check probabilities for\n","  mask_words_list = [\"he\", \"she\"]\n","\n","  # Get the probabilities of specified words\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  if word_probabilities['he'] - word_probabilities['she'] < 0.010:\n","    possible_values.append([scalar_values, round(word_probabilities['he'] - word_probabilities['she'],4)])\n","\n","import random\n","\n","for i in range(50):\n","  original_list = [1,1,1,1,1,1,1,1,1,1,1,1]\n","  scalar_values = [random.choice([0.01,0.1,1,10]) for _ in original_list]\n","  run()\n","\n","for value in possible_values:\n","  print(value[0],'\\t',value[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":362,"referenced_widgets":["23c71562dc8447b59713c3f54986b362","4a2e83b07e8d44d081c22333c6b864d8","29a99136518541668bc8291010ed27f6","d84a2793f7db4c16901e8004fac61624","20b1c6ecd987447697a48c25d63924b1","60d56824011b4bd981ddb5f040715f26","eaaee47cd66e4a38a094f3237cfed3be","401199fc61a54cb596f547b7bb730a88","57dc9627371c46eeab696b945eab7613","b5713f476853466490077551f03dea22","5f298a762ecb4048ba2efac77df139c7","5639161f9aeb4d74bef54a7ac4dde7dc","dfea58ffa897480483dcc59f16d9babe","c61cb545b55a4429bf0a5cd6546f621f","7432eb59df3e40ca9c13544b85309ad6","189c0e7825b44b85ae64cbea9e870e2e","5d5c44b7335340f4bedf847d869db43e","82b3ad8f86204595a94233ee586a579d","8b0a9ad2962346aab16ad3d81bfe0c00","4ca216aa70ed4dabb0721d29eb30914a","4d792e1ec84e450788e529265b24dc08","bdddb1851cc846b0a9c12e8ac85148eb","c28f0d78d18044d09e110398e559b273","e13d5efcb69b4dd6b42d5d66385320df","ecde37c7ec7b47cdae6884b888095e00","82d690f61d5347c0acec7e20b38fff13","b040a46c1e804835b429f8219cf72ffb","3194ece35dd942d187f2e4fc4d690119","6cdbcf3fc7d9468196bf4e436f2f1bb7","1ba9894acff342b3aa31311fffcde308","7f9face092864cddaae69f20822f4003","0f603b52c8a341e29e99570bf2893715","494ea026ce51485980293fcda0c6b71b","8c8aa529fb344ff3810548433034bd59","f162469cda254e59ac1333b22103f64e","7e1977c1c4c54e07a1d2deaae279b088","5b5b789248cc4f2d86bbb034e1f261d1","174eb98ea8dd483894a3d71e1bb2cbad","91dc928336514813a5b0240acfff6450","0d3c794381154e0a9000b28d4f1726aa","92d923062e34424b9c88d81082eb6c02","8edb94743b5d4c4ba99e89e18841b086","3026f29344894c8b8447a2f443142ee1","06be9fdd2dcb4883829eac90d2f7d14d","a10cf04f4eb14ef9aa8ef597bd55718f","52f700b623bb482e8b83bbb1d9c30abe","82be3d8ca6734da7b1fd1ffd13be17b7","caae60c12ada4e32a223d5f9257899bd","bec84dd73ff049eba5267ff97f070fa4","2cf5673954b34fe6a17fb0312eb603cb","613984f4c3b649779627df8116b44497","dfe6196a4121463db87677c53a31323e","faaadcd65cb243c697ebc669bf172853","f109bb6b07d340d8884140a4fe608595","b4eef3ee5e6c4089a06b6dcb749185e7"]},"executionInfo":{"elapsed":207276,"status":"ok","timestamp":1713771312727,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"Y3RC-8GMIwkz","outputId":"4f480d71-41d8-4731-e8c7-6950d137f890"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23c71562dc8447b59713c3f54986b362","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5639161f9aeb4d74bef54a7ac4dde7dc","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c28f0d78d18044d09e110398e559b273","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c8aa529fb344ff3810548433034bd59","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a10cf04f4eb14ef9aa8ef597bd55718f","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[0.01, 0.01, 0.1, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 1, 0.1, 10] \t 0.008\n","[0.1, 0.1, 0.01, 1, 0.01, 0.1, 0.1, 0.1, 0.1, 10, 10, 0.1] \t 0.0021\n","[1, 0.01, 0.01, 0.01, 0.1, 0.01, 0.01, 0.1, 10, 10, 1, 10] \t 0.0018\n"]}],"source":["possible_values = []\n","\n","def run():\n","  # Example sentence\n","  sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","  # List of words to check probabilities for\n","  mask_words_list = [\"he\", \"she\"]\n","\n","  # Get the probabilities of specified words\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  if word_probabilities['he'] - word_probabilities['she'] < 0.010:\n","    possible_values.append([scalar_values, round(word_probabilities['he'] - word_probabilities['she'],4)])\n","\n","import random\n","\n","for i in range(150):\n","  original_list = [1,1,1,1,1,1,1,1,1,1,1,1]\n","  scalar_values = [random.choice([0.01,0.1,1,10]) for _ in original_list]\n","  run()\n","\n","for value in possible_values:\n","  print(value[0],'\\t',value[1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzIParr6LREB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":264125,"status":"ok","timestamp":1713611374104,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"JvpHyb7ALRH6","outputId":"189563d5-d841-4a9b-c4a6-d7d792cd5747"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.03995127230882645\n","she: 0.04162386432290077\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = [0.1, 0.1, 0.1, 0.01, 1, 0.1, 0.1, 0.1, 0.01, 10, 10, 10]\n","Winogender - Average gender bias in bert:  0.37\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.1, 0.1, 0.1, 0.01, 1, 0.1, 0.1, 0.1, 0.01, 10, 10, 10]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = [0.1, 0.1, 0.1, 0.01, 1, 0.1, 0.1, 0.1, 0.01, 10, 10, 10]')\n","print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262965,"status":"ok","timestamp":1713611637063,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"WtX3Y-V3LjyM","outputId":"0777401a-c4cd-43b1-80b9-e2eb43c93073"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.0041360617615282536\n","she: 0.0017532843630760908\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = [10, 0.1, 0.1, 0.01, 0.01, 0.1, 0.1, 0.1, 10, 10, 0.1, 10]\n","Winogender - Average gender bias in bert:  0.39\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["\n","# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [10, 0.1, 0.1, 0.01, 0.01, 0.1, 0.1, 0.1, 10, 10, 0.1, 10]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = [10, 0.1, 0.1, 0.01, 0.01, 0.1, 0.1, 0.1, 10, 10, 0.1, 10]')\n","print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265460,"status":"ok","timestamp":1713611952353,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"7puqPZ2oL2j9","outputId":"79b82093-328a-4d36-eba8-47adcbaecb72"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.007070059422403574\n","she: 0.0044663515873253345\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = [1, 0.01, 0.01, 0.01, 0.01, 0.1, 0.01, 0.1, 10, 0.01, 1, 0.1] \n","Winogender - Average gender bias in bert:  0.17\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["\n","# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [1, 0.01, 0.01, 0.01, 0.01, 0.1, 0.01, 0.1, 10, 0.01, 1, 0.1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = [1, 0.01, 0.01, 0.01, 0.01, 0.1, 0.01, 0.1, 10, 0.01, 1, 0.1] ')\n","print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278428,"status":"ok","timestamp":1713612230767,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"moH3KFdYNpFG","outputId":"22f0cecc-ec20-43b2-97a6-17475061b4f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.0046435813419520855\n","she: 0.0019269033800810575\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = [10, 0.01, 0.01, 0.1, 0.01, 0.01, 0.1, 0.1, 10, 10, 1, 0.01]\n","Winogender - Average gender bias in bert:  0.29\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["\n","# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [10, 0.01, 0.01, 0.1, 0.01, 0.01, 0.1, 0.1, 10, 10, 1, 0.01]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = [10, 0.01, 0.01, 0.1, 0.01, 0.01, 0.1, 0.1, 10, 10, 1, 0.01]')\n","print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304897,"status":"ok","timestamp":1713612535658,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"CjLv2PsTN0Z_","outputId":"91c888cb-903f-4cf9-e960-8113c7751750"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.010791823267936707\n","she: 0.0074132876470685005\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = [0.01, 0.1, 0.01, 0.1, 0.1, 0.01, 0.01, 0.01, 0.1, 0.01, 1, 1]\n","Winogender - Average gender bias in bert:  0.2\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["\n","# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.01, 0.1, 0.01, 0.1, 0.1, 0.01, 0.01, 0.01, 0.1, 0.01, 1, 1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = [0.01, 0.1, 0.01, 0.1, 0.1, 0.01, 0.01, 0.01, 0.1, 0.01, 1, 1]')\n","print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":250877,"status":"ok","timestamp":1713613467021,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"TjmNXjgeTase","outputId":"bba66a4f-f76a-4219-de4f-1c79f2aea32b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.010477169416844845\n","she: 0.006977181416004896\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = [0.01, 0.1, 0.01, 0.01, 0.01, 0.1, 0.1, 1, 0.01, 0.1, 10, 1]\n","Winogender - Average gender bias in bert:  0.25\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["\n","# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.01, 0.1, 0.01, 0.01, 0.01, 0.1, 0.1, 1, 0.01, 0.1, 10, 1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = [0.01, 0.1, 0.01, 0.01, 0.01, 0.1, 0.1, 1, 0.01, 0.1, 10, 1]')\n","print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":250397,"status":"ok","timestamp":1713613717413,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"cRguwVAiTalJ","outputId":"b64c3d82-b3af-4a12-c9ce-12ad68759ded"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.0015088701620697975\n","she: 0.0004904202651232481\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = [0.1, 0.01, 0.1, 0.1, 0.1, 0.1, 10, 1, 1, 10, 10, 1] \n","Winogender - Average gender bias in bert:  0.41\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["\n","# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [0.1, 0.01, 0.1, 0.1, 0.1, 0.1, 10, 1, 1, 10, 10, 1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = [0.1, 0.01, 0.1, 0.1, 0.1, 0.1, 10, 1, 1, 10, 10, 1] ')\n","print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZNeYfmDN9Ou"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q24Aco6HPn4q"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jdSaIVs-Pnwa"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZX4hAGVnTUlG"},"source":["# ***Grid Search***"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["54bef6a20bd14e0b80816d072f1edadf","c6b1cc93b2da4fd5a90fab204f50e2a8","98c6a7ef5ac14ae6975813b41bb67fde","d2a2edb38f4a494eb22a374ec78faa2d","25f7c50b7b4940d4b9425c88db5927c0"]},"executionInfo":{"elapsed":13506781,"status":"error","timestamp":1713668678921,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"75e2bor9Pnrv","outputId":"78d5a316-ad7c-4963-98ff-65c002551c89"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54bef6a20bd14e0b80816d072f1edadf","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6b1cc93b2da4fd5a90fab204f50e2a8","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98c6a7ef5ac14ae6975813b41bb67fde","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2a2edb38f4a494eb22a374ec78faa2d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25f7c50b7b4940d4b9425c88db5927c0","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.5355629324913025\n","she: 0.02876003459095955\n","Mounted at /content/drive\n","-----------------------------------------------\n","Beta =  [0.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.58\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5858899354934692\n","she: 0.027688665315508842\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.58\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5812698602676392\n","she: 0.01871231384575367\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.56\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5604868531227112\n","she: 0.02969399467110634\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.54\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5604868531227112\n","she: 0.02969399467110634\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.54\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5604868531227112\n","she: 0.02969399467110634\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.54\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5604868531227112\n","she: 0.02969399467110634\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.54\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5612223148345947\n","she: 0.02927692048251629\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 0.1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5612223148345947\n","she: 0.02927692048251629\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 0.1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5612223148345947\n","she: 0.02927692048251629\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 0.1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5612223148345947\n","she: 0.02927692048251629\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 0.1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5812698602676392\n","she: 0.01871231384575367\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.56\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5812698602676392\n","she: 0.01871231384575367\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.56\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5812698602676392\n","she: 0.01871231384575367\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.56\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5812698602676392\n","she: 0.01871231384575367\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.56\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.6328215599060059\n","she: 0.02279571257531643\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 10, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.54\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.6328215599060059\n","she: 0.02279571257531643\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 10, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.54\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.6328215599060059\n","she: 0.02279571257531643\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.01, 10, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.54\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.6346343159675598\n","she: 0.022440167143940926\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 10, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5554817318916321\n","she: 0.029997285455465317\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5554817318916321\n","she: 0.029997285455465317\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5554817318916321\n","she: 0.029997285455465317\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5554817318916321\n","she: 0.029997285455465317\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5534578561782837\n","she: 0.02792426384985447\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 0.1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5534578561782837\n","she: 0.02792426384985447\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 0.1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5534578561782837\n","she: 0.02792426384985447\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 0.1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5534578561782837\n","she: 0.02792426384985447\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 0.1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.6047196388244629\n","she: 0.018277466297149658\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.57\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.6047196388244629\n","she: 0.018277466297149658\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.57\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.6047196388244629\n","she: 0.018277466297149658\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.57\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.6047196388244629\n","she: 0.018277466297149658\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.57\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.6346343159675598\n","she: 0.022440167143940926\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 10, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.6346343159675598\n","she: 0.022440167143940926\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 10, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.6346343159675598\n","she: 0.022440167143940926\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 0.1, 10, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5656996965408325\n","she: 0.0251919012516737\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 1, 10, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.57\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5639916062355042\n","she: 0.02940797433257103\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 1, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.54\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5639916062355042\n","she: 0.02940797433257103\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 1, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.54\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5639916062355042\n","she: 0.02940797433257103\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 1, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.54\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.5639916062355042\n","she: 0.02940797433257103\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 1, 0.01, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.54\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.571553647518158\n","she: 0.028239542618393898\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 1, 0.1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.571553647518158\n","she: 0.028239542618393898\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 1, 0.1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.571553647518158\n","she: 0.028239542618393898\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta =  [0.01, 0.01, 1, 0.1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.55\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n","Probabilities of specified words:\n","he: 0.571553647518158\n","she: 0.028239542618393898\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-6198c1a48ebc>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Apply the function to each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mdf_winogender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_winogender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0maverage_bias_winogender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_winogender\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bias_percent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9422\u001b[0m         )\n\u001b[0;32m-> 9423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9425\u001b[0m     def applymap(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-6198c1a48ebc>\u001b[0m in \u001b[0;36mapply_bert\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     47\u001b[0m           \u001b[0mmask_words_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'male_pronoun'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'female_pronoun'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m           \u001b[0mword_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_masked_token_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_words_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m           \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'male_prob_abs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'female_prob_abs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'male_pronoun'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_probabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'female_pronoun'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-e4c902438528>\u001b[0m in \u001b[0;36mget_masked_token_probabilities\u001b[0;34m(sentence, mask_words_list)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_masked_token_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_words_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Modify attention heads with scalar values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3500\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3501\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3502\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3503\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3867\u001b[0m                 \u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3868\u001b[0m             )\n\u001b[0;32m-> 3869\u001b[0;31m             \u001b[0merror_msgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_state_dict_into_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3870\u001b[0m             \u001b[0moffload_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3871\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m     \u001b[0;31m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;31m# it's safe to delete it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    618\u001b[0m                             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_from_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_from_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                                 \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m                             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2041\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m                     error_msgs.append(f'While copying the parameter named \"{key}\", '\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# from itertools import product\n","\n","# positions = [1, 1, 1, 1]\n","# values = [0.01, 0.1, 1, 10]\n","\n","# combinations = list(product(values, repeat=len(positions)))\n","\n","# for combo in combinations:\n","#     for i, value in enumerate(combo):\n","#         positions[i] = value\n","\n","#         new_positions = positions + [1,1,1,1,1,1,1,1]\n","\n","#         # Example sentence\n","#         sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","#         # List of words to check probabilities for\n","#         mask_words_list = [\"he\", \"she\"]\n","\n","#         # Scalar values for modifying attention heads\n","#         scalar_values = positions\n","\n","\n","#         # Get the probabilities of specified words\n","#         word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","#         print(\"Probabilities of specified words:\")\n","#         for word, probability in word_probabilities.items():\n","#             print(f\"{word}: {probability}\")\n","\n","#         from google.colab import drive\n","#         drive.mount('/content/drive')\n","\n","#         import pandas as pd\n","#         pd.set_option('display.max_colwidth', None)\n","\n","#         # Read the CSV file into a pandas DataFrame\n","\n","#         df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","#         new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","#         for col_name in new_cols:\n","#             df_winogender[col_name] = None\n","\n","#         def apply_bert(row):\n","#           sentence = row['masked_sentence']\n","#           mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","#           word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","#           row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","#           row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","#           row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","#           row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","#           return row\n","\n","\n","#         # Apply the function to each row\n","#         df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","#         average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","#         print('-----------------------------------------------')\n","#         print('Beta = ', new_positions)\n","#         print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","#         print('-----------------------------------------------')\n","#         print('Score 0 : No bias')\n","#         print('Score 1 : Complete bias towards one gender')\n","#         print('-----------------------------------------------')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333674,"status":"ok","timestamp":1713723008613,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"Pwp0iCgjnQzm","outputId":"9e4f7266-9f9c-4830-979b-ec9a88811b88"},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------\n"]}],"source":["possible_values = []\n","\n","from itertools import product\n","\n","positions = [1, 1, 1, 1]\n","values = [0.01, 0.1, 1, 10]\n","\n","combinations = list(product(values, repeat=len(positions)))\n","\n","# for combo in combinations:\n","#     for i, value in enumerate(combo):\n","#         positions[i] = value\n","\n","#         new_position = positions + [1,1,1,1,1,1,1,1]\n","\n","#         # Example sentence\n","#         sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","#         # List of words to check probabilities for\n","#         mask_words_list = [\"he\", \"she\"]\n","\n","#         # Scalar values for modifying attention heads\n","#         scalar_values = new_position\n","\n","#         # Get the probabilities of specified words\n","#         word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","#         if word_probabilities['he'] - word_probabilities['she'] < 0.10:\n","#           possible_values.append([scalar_values, round(word_probabilities['he'] - word_probabilities['she'],4)])\n","\n","for combo in combinations:\n","\n","  new_position = list(combo) + [1,1,1,1,1,1,1,1]\n","\n","  # Example sentence\n","  sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","  # List of words to check probabilities for\n","  mask_words_list = [\"he\", \"she\"]\n","\n","  # Scalar values for modifying attention heads\n","  scalar_values = new_position\n","\n","  # Get the probabilities of specified words\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  if abs(word_probabilities['he'] - word_probabilities['she']) < 0.10:\n","    possible_values.append([scalar_values, round(word_probabilities['he'] - word_probabilities['she'],4)])\n","\n","\n","for value in possible_values:\n","  print(value[0],'\\t',value[1])\n","print('---------------')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":332891,"status":"ok","timestamp":1713723341490,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"drxXffRKpNsQ","outputId":"164f5f32-2be0-40b2-f0cc-50206c558824"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 1, 1, 1, 0.01, 0.01, 0.01, 0.01, 1, 1, 1, 1] \t 0.0815\n","[1, 1, 1, 1, 0.01, 0.01, 0.1, 0.01, 1, 1, 1, 1] \t 0.0981\n","---------------\n"]}],"source":["possible_values = []\n","\n","from itertools import product\n","\n","positions = [1, 1, 1, 1]\n","values = [0.01, 0.1, 1, 10]\n","\n","combinations = list(product(values, repeat=len(positions)))\n","\n","# for combo in combinations:\n","#     for i, value in enumerate(combo):\n","#         positions[i] = value\n","\n","#         new_positions = [1,1,1,1] + positions + [1,1,1,1]\n","\n","#         # Example sentence\n","#         sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","#         # List of words to check probabilities for\n","#         mask_words_list = [\"he\", \"she\"]\n","\n","#         # Scalar values for modifying attention heads\n","#         scalar_values = new_positions\n","\n","#         # Get the probabilities of specified words\n","#         word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","#         if word_probabilities['he'] - word_probabilities['she'] < 0.10:\n","#           possible_values.append([scalar_values, round(word_probabilities['he'] - word_probabilities['she'],4)])\n","\n","for combo in combinations:\n","\n","  new_position = [1,1,1,1] + list(combo) + [1,1,1,1]\n","\n","  # Example sentence\n","  sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","  # List of words to check probabilities for\n","  mask_words_list = [\"he\", \"she\"]\n","\n","  # Scalar values for modifying attention heads\n","  scalar_values = new_position\n","\n","  # Get the probabilities of specified words\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  if abs(word_probabilities['he'] - word_probabilities['she']) < 0.10:\n","    possible_values.append([scalar_values, round(word_probabilities['he'] - word_probabilities['she'],4)])\n","\n","\n","for value in possible_values:\n","  print(value[0],'\\t',value[1])\n","print('---------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334916,"status":"ok","timestamp":1713723676396,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"ra6kdmsNpW2z","outputId":"f932774b-4957-4aca-ad87-6180de13bd13"},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------\n"]}],"source":["possible_values = []\n","\n","from itertools import product\n","\n","positions = [1, 1, 1, 1]\n","values = [0.01, 0.1, 1, 10]\n","\n","combinations = list(product(values, repeat=len(positions)))\n","\n","# for combo in combinations:\n","#     for i, value in enumerate(combo):\n","#         positions[i] = value\n","\n","#         new_position = [1,1,1,1,1,1,1,1] + positions\n","\n","#         # Example sentence\n","#         sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","#         # List of words to check probabilities for\n","#         mask_words_list = [\"he\", \"she\"]\n","\n","#         # Scalar values for modifying attention heads\n","#         scalar_values = new_position\n","\n","#         # Get the probabilities of specified words\n","#         word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","#         if word_probabilities['he'] - word_probabilities['she'] < 0.10:\n","#           possible_values.append([scalar_values, round(word_probabilities['he'] - word_probabilities['she'],4)])\n","\n","for combo in combinations:\n","\n","  new_position = [1,1,1,1,1,1,1,1] + list(combo)\n","\n","  # Example sentence\n","  sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","  # List of words to check probabilities for\n","  mask_words_list = [\"he\", \"she\"]\n","\n","  # Scalar values for modifying attention heads\n","  scalar_values = new_position\n","\n","  # Get the probabilities of specified words\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  if abs(word_probabilities['he'] - word_probabilities['she']) < 0.10:\n","    possible_values.append([scalar_values, round(word_probabilities['he'] - word_probabilities['she'],4)])\n","\n","\n","for value in possible_values:\n","  print(value[0],'\\t',value[1])\n","print('---------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334955,"status":"ok","timestamp":1713728882531,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"0E9fTJieLP9r","outputId":"e4f19fc9-362c-4954-dc48-f52c57f41e28"},"outputs":[{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.1055576354265213\n","she: 0.02404152601957321\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-----------------------------------------------\n","Beta = [1, 1, 1, 1, 0.01, 0.01, 0.01, 0.01, 1, 1, 1, 1]\n","Winogender - Average gender bias in bert:  0.38\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [1, 1, 1, 1, 0.01, 0.01, 0.01, 0.01, 1, 1, 1, 1]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = [1, 1, 1, 1, 0.01, 0.01, 0.01, 0.01, 1, 1, 1, 1]')\n","print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLxD8qKERFyq"},"outputs":[],"source":["# from itertools import product\n","\n","# positions = [1, 1, 1, 1]\n","# values = [0.01, 0.1, 1, 10]\n","\n","# combinations = list(product(values, repeat=len(positions)))\n","\n","# for combo in combinations:\n","#     for i, value in enumerate(combo):\n","#         positions[i] = value\n","\n","#         new_positions = [1,1,1,1] + positions + [1,1,1,1]\n","\n","\n","#         # Example sentence\n","#         sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","#         # List of words to check probabilities for\n","#         mask_words_list = [\"he\", \"she\"]\n","\n","#         # Scalar values for modifying attention heads\n","#         scalar_values = new_positions\n","\n","\n","#         # Get the probabilities of specified words\n","#         word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","#         print(\"Probabilities of specified words:\")\n","#         for word, probability in word_probabilities.items():\n","#             print(f\"{word}: {probability}\")\n","\n","#         from google.colab import drive\n","#         drive.mount('/content/drive')\n","\n","#         import pandas as pd\n","#         pd.set_option('display.max_colwidth', None)\n","\n","#         # Read the CSV file into a pandas DataFrame\n","\n","#         df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","#         new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","#         for col_name in new_cols:\n","#             df_winogender[col_name] = None\n","\n","#         def apply_bert(row):\n","#           sentence = row['masked_sentence']\n","#           mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","#           word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","#           row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","#           row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","#           row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","#           row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","#           return row\n","\n","\n","#         # Apply the function to each row\n","#         df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","#         average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","#         print('-----------------------------------------------')\n","#         print('Beta = ', new_positions)\n","#         print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","#         print('-----------------------------------------------')\n","#         print('Score 0 : No bias')\n","#         print('Score 1 : Complete bias towards one gender')\n","#         print('-----------------------------------------------')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMEYjPoTSAXj"},"outputs":[],"source":["\n","# from itertools import product\n","\n","# positions = [1, 1, 1, 1]\n","# values = [0.01, 0.1, 1, 10]\n","\n","# combinations = list(product(values, repeat=len(positions)))\n","\n","# for combo in combinations:\n","#     for i, value in enumerate(combo):\n","#         positions[i] = value\n","\n","#         new_positions = [1,1,1,1,1,1,1,1] + positions\n","\n","\n","#         # Example sentence\n","#         sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","#         # List of words to check probabilities for\n","#         mask_words_list = [\"he\", \"she\"]\n","\n","#         # Scalar values for modifying attention heads\n","#         scalar_values = new_positions\n","\n","\n","#         # Get the probabilities of specified words\n","#         word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","#         print(\"Probabilities of specified words:\")\n","#         for word, probability in word_probabilities.items():\n","#             print(f\"{word}: {probability}\")\n","\n","#         from google.colab import drive\n","#         drive.mount('/content/drive')\n","\n","#         import pandas as pd\n","#         pd.set_option('display.max_colwidth', None)\n","\n","#         # Read the CSV file into a pandas DataFrame\n","\n","#         df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","#         new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","#         for col_name in new_cols:\n","#             df_winogender[col_name] = None\n","\n","#         def apply_bert(row):\n","#           sentence = row['masked_sentence']\n","#           mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","#           word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","#           row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","#           row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","#           row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","#           row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","#           return row\n","\n","\n","#         # Apply the function to each row\n","#         df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","#         average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","#         print('-----------------------------------------------')\n","#         print('Beta = ', new_positions)\n","#         print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","#         print('-----------------------------------------------')\n","#         print('Score 0 : No bias')\n","#         print('Score 1 : Complete bias towards one gender')\n","#         print('-----------------------------------------------')\n"]},{"cell_type":"markdown","metadata":{"id":"VM_4WwJUDRzh"},"source":["\n","# ***Bayesian Optimization***\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":577},"executionInfo":{"elapsed":17266,"status":"error","timestamp":1713676284289,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"M1ZvixKzW_GR","outputId":"800e0bb7-bba4-49b3-9199-bbced455de2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: GPyOpt in /usr/local/lib/python3.10/dist-packages (1.2.6)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from GPyOpt) (1.25.2)\n","Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.10/dist-packages (from GPyOpt) (1.11.4)\n","Requirement already satisfied: GPy>=1.8 in /usr/local/lib/python3.10/dist-packages (from GPyOpt) (1.13.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from GPy>=1.8->GPyOpt) (1.16.0)\n","Requirement already satisfied: paramz>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from GPy>=1.8->GPyOpt) (0.9.6)\n","Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.10/dist-packages (from GPy>=1.8->GPyOpt) (3.0.10)\n","Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from paramz>=0.9.6->GPy>=1.8->GPyOpt) (4.4.2)\n"]},{"ename":"ValueError","evalue":"operands could not be broadcast together with shapes (768,768) (12,) ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-76451ba4be91>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Bayesian optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquisition_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquisition_jitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Initial random points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, domain, constraints, cost_withGradients, model_type, X, Y, initial_design_numdata, initial_design_type, acquisition_type, normalize_Y, exact_feval, acquisition_optimizer_type, model_update_interval, evaluator_type, batch_size, num_cores, verbosity, verbosity_model, maximize, de_duplication, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0minitial_design_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_design_chooser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# --- CHOOSE the model type. If an instance of a GPyOpt model is passed (possibly user defined), it is used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m_init_design_chooser\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Case 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_procs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36m_eval_func\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mst_time\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mrlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mf_evals\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrlt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mcost_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-76451ba4be91>\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(scalar_values)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Get the probabilities of specified words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mword_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_masked_token_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_words_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Difference between the probabilities of 'he' and 'she'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-76451ba4be91>\u001b[0m in \u001b[0;36mget_masked_token_probabilities\u001b[0;34m(sentence, mask_words_list, scalar_values)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Modify attention heads with scalar values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodify_attention_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-76451ba4be91>\u001b[0m in \u001b[0;36mmodify_attention_heads\u001b[0;34m(model, scalar_values)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mbias_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'bert.encoder.layer.{layer_num}.attention.self.{qkv}.bias'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mscalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbias_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mscalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (768,768) (12,) "]}],"source":["# ! pip install GPyOpt\n","\n","# import torch\n","# from transformers import BertTokenizer, BertForMaskedLM\n","# from GPyOpt.methods import BayesianOptimization\n","\n","# def modify_attention_heads(model, scalar_values):\n","#     model_dict = model.state_dict()\n","\n","#     # Identify the keys corresponding to the attention heads\n","#     attention_keys = [key for key in model_dict.keys() if 'attention.self.query.weight' in key]\n","\n","#     # Modify each attention head with scalar values\n","#     for key, scalar in zip(attention_keys, scalar_values):\n","#         # Extract the layer number\n","#         layer_num = key.split('bert.encoder.layer.')[1].split('.')[0]\n","\n","#         # Update query, key, and value weights\n","#         query_key_value = ['query', 'key', 'value']\n","\n","#         for qkv in query_key_value:\n","#             weight_key = f'bert.encoder.layer.{layer_num}.attention.self.{qkv}.weight'\n","#             bias_key = f'bert.encoder.layer.{layer_num}.attention.self.{qkv}.bias'\n","\n","#             model_dict[weight_key] *= scalar\n","#             model_dict[bias_key] *= scalar\n","\n","#     model.load_state_dict(model_dict)\n","#     return model\n","\n","\n","# def objective_function(scalar_values):\n","#     sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","#     mask_words_list = [\"he\", \"she\"]\n","\n","#     # Get the probabilities of specified words\n","#     word_probabilities = get_masked_token_probabilities(sentence, mask_words_list, scalar_values)\n","\n","#     # Difference between the probabilities of 'he' and 'she'\n","#     diff = abs(word_probabilities[\"he\"] - word_probabilities[\"she\"])\n","\n","#     return -diff  # Minimize the difference\n","\n","# # Function to get masked token probabilities\n","# def get_masked_token_probabilities(sentence, mask_words_list, scalar_values):\n","#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","#     model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","\n","#     # Modify attention heads with scalar values\n","#     model = modify_attention_heads(model, scalar_values)\n","\n","#     inputs = tokenizer(sentence, return_tensors=\"pt\")\n","#     mask_token_index = torch.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)[0]\n","\n","#     # Ensure only one mask token in the sentence\n","#     if len(mask_token_index) != 1:\n","#         raise ValueError(\"Please provide a sentence with exactly one [MASK] token.\")\n","\n","#     mask_token_index = mask_token_index.item()\n","\n","#     with torch.no_grad():\n","#         outputs = model(**inputs)\n","\n","#     # Get logits for mask token\n","#     logits = outputs.logits\n","#     mask_token_logits = logits[0, mask_token_index, :]\n","\n","#     # Calculate probabilities\n","#     probabilities = torch.softmax(mask_token_logits, dim=0)\n","\n","#     # Convert token ids to words\n","#     mask_token_id = torch.argmax(probabilities).item()\n","#     mask_word = tokenizer.convert_ids_to_tokens(mask_token_id)\n","\n","#     # Filter probabilities for mask_words_list\n","#     word_probabilities = {word: probabilities[tokenizer.convert_tokens_to_ids(word)].item() for word in mask_words_list}\n","\n","#     return word_probabilities\n","\n","# # Bounds for the scalar values\n","# bounds = [{'name': f'scalar_{i}', 'type': 'continuous', 'domain': (0.01, 10)} for i in range(12)]\n","\n","# # Bayesian optimization\n","# optimizer = BayesianOptimization(f=objective_function, domain=bounds, model_type='GP', acquisition_type='EI', acquisition_jitter=0.01, maximize=False)\n","\n","# # Initial random points\n","# optimizer.run_optimization(max_iter=20)\n","\n","# # Best scalar values\n","# best_scalar_values = optimizer.x_opt\n","# print(\"Best scalar values for each attention head:\", best_scalar_values)\n","\n","# # Get the probabilities of specified words with best scalar values\n","# best_word_probabilities = get_masked_token_probabilities(sentence, mask_words_list, best_scalar_values)\n","\n","# print(\"\\nProbabilities of specified words with best scalar values:\")\n","# for word, probability in best_word_probabilities.items():\n","#     print(f\"{word}: {probability}\")\n"]},{"cell_type":"markdown","metadata":{"id":"NxG5Vl2dp3pg"},"source":["# ***Bayesian Optimization for just one sentence***"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110527,"status":"ok","timestamp":1713676475143,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"dYt54LwYDaLW","outputId":"4b9376a5-76a9-410b-bf5a-fca56a0b403a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting bayesian-optimization\n","  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.25.2)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n","Collecting colorama>=0.4.6 (from bayesian-optimization)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.4.0)\n","Installing collected packages: colorama, bayesian-optimization\n","Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n","|   iter    |  target   | scalar_0  | scalar_1  | scalar_10 | scalar_11 | scalar_2  | scalar_3  | scalar_4  | scalar_5  | scalar_6  | scalar_7  | scalar_8  | scalar_9  |\n","-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","| \u001b[0m1        \u001b[0m | \u001b[0m-0.1628  \u001b[0m | \u001b[0m4.176    \u001b[0m | \u001b[0m7.206    \u001b[0m | \u001b[0m0.01114  \u001b[0m | \u001b[0m3.03     \u001b[0m | \u001b[0m1.476    \u001b[0m | \u001b[0m0.9325   \u001b[0m | \u001b[0m1.871    \u001b[0m | \u001b[0m3.462    \u001b[0m | \u001b[0m3.974    \u001b[0m | \u001b[0m5.393    \u001b[0m | \u001b[0m4.198    \u001b[0m | \u001b[0m6.855    \u001b[0m |\n","| \u001b[0m2        \u001b[0m | \u001b[0m-0.1813  \u001b[0m | \u001b[0m2.052    \u001b[0m | \u001b[0m8.782    \u001b[0m | \u001b[0m0.2836   \u001b[0m | \u001b[0m6.708    \u001b[0m | \u001b[0m4.179    \u001b[0m | \u001b[0m5.591    \u001b[0m | \u001b[0m1.412    \u001b[0m | \u001b[0m1.989    \u001b[0m | \u001b[0m8.009    \u001b[0m | \u001b[0m9.683    \u001b[0m | \u001b[0m3.141    \u001b[0m | \u001b[0m6.926    \u001b[0m |\n","| \u001b[0m3        \u001b[0m | \u001b[0m-0.4893  \u001b[0m | \u001b[0m8.765    \u001b[0m | \u001b[0m8.947    \u001b[0m | \u001b[0m0.8596   \u001b[0m | \u001b[0m0.4002   \u001b[0m | \u001b[0m1.707    \u001b[0m | \u001b[0m8.783    \u001b[0m | \u001b[0m0.9925   \u001b[0m | \u001b[0m4.217    \u001b[0m | \u001b[0m9.579    \u001b[0m | \u001b[0m5.336    \u001b[0m | \u001b[0m6.922    \u001b[0m | \u001b[0m3.162    \u001b[0m |\n","| \u001b[95m4        \u001b[0m | \u001b[95m-0.1335  \u001b[0m | \u001b[95m6.868    \u001b[0m | \u001b[95m8.348    \u001b[0m | \u001b[95m0.1927   \u001b[0m | \u001b[95m7.504    \u001b[0m | \u001b[95m9.889    \u001b[0m | \u001b[95m7.484    \u001b[0m | \u001b[95m2.812    \u001b[0m | \u001b[95m7.895    \u001b[0m | \u001b[95m1.041    \u001b[0m | \u001b[95m4.484    \u001b[0m | \u001b[95m9.087    \u001b[0m | \u001b[95m2.943    \u001b[0m |\n","| \u001b[0m5        \u001b[0m | \u001b[0m-0.425   \u001b[0m | \u001b[0m2.885    \u001b[0m | \u001b[0m1.309    \u001b[0m | \u001b[0m0.2035   \u001b[0m | \u001b[0m6.792    \u001b[0m | \u001b[0m2.124    \u001b[0m | \u001b[0m2.663    \u001b[0m | \u001b[0m4.921    \u001b[0m | \u001b[0m0.5431   \u001b[0m | \u001b[0m5.745    \u001b[0m | \u001b[0m1.476    \u001b[0m | \u001b[0m5.897    \u001b[0m | \u001b[0m7.001    \u001b[0m |\n","| \u001b[0m6        \u001b[0m | \u001b[0m-0.8156  \u001b[0m | \u001b[0m1.032    \u001b[0m | \u001b[0m4.146    \u001b[0m | \u001b[0m6.947    \u001b[0m | \u001b[0m4.148    \u001b[0m | \u001b[0m0.509    \u001b[0m | \u001b[0m5.364    \u001b[0m | \u001b[0m6.641    \u001b[0m | \u001b[0m5.154    \u001b[0m | \u001b[0m9.447    \u001b[0m | \u001b[0m5.87     \u001b[0m | \u001b[0m9.035    \u001b[0m | \u001b[0m1.383    \u001b[0m |\n","| \u001b[0m7        \u001b[0m | \u001b[0m-0.4059  \u001b[0m | \u001b[0m1.401    \u001b[0m | \u001b[0m8.076    \u001b[0m | \u001b[0m3.983    \u001b[0m | \u001b[0m1.662    \u001b[0m | \u001b[0m9.276    \u001b[0m | \u001b[0m3.484    \u001b[0m | \u001b[0m7.511    \u001b[0m | \u001b[0m7.263    \u001b[0m | \u001b[0m8.834    \u001b[0m | \u001b[0m6.24     \u001b[0m | \u001b[0m7.512    \u001b[0m | \u001b[0m3.495    \u001b[0m |\n","| \u001b[0m8        \u001b[0m | \u001b[0m-0.3431  \u001b[0m | \u001b[0m2.707    \u001b[0m | \u001b[0m8.96     \u001b[0m | \u001b[0m4.287    \u001b[0m | \u001b[0m9.649    \u001b[0m | \u001b[0m6.638    \u001b[0m | \u001b[0m6.221    \u001b[0m | \u001b[0m1.156    \u001b[0m | \u001b[0m9.495    \u001b[0m | \u001b[0m4.505    \u001b[0m | \u001b[0m5.788    \u001b[0m | \u001b[0m4.087    \u001b[0m | \u001b[0m2.378    \u001b[0m |\n","| \u001b[0m9        \u001b[0m | \u001b[0m-0.5537  \u001b[0m | \u001b[0m9.035    \u001b[0m | \u001b[0m5.741    \u001b[0m | \u001b[0m0.03867  \u001b[0m | \u001b[0m6.175    \u001b[0m | \u001b[0m3.273    \u001b[0m | \u001b[0m5.275    \u001b[0m | \u001b[0m8.861    \u001b[0m | \u001b[0m3.579    \u001b[0m | \u001b[0m9.086    \u001b[0m | \u001b[0m6.237    \u001b[0m | \u001b[0m0.1681   \u001b[0m | \u001b[0m9.295    \u001b[0m |\n","| \u001b[0m10       \u001b[0m | \u001b[0m-0.6252  \u001b[0m | \u001b[0m6.912    \u001b[0m | \u001b[0m9.973    \u001b[0m | \u001b[0m1.732    \u001b[0m | \u001b[0m1.38     \u001b[0m | \u001b[0m9.327    \u001b[0m | \u001b[0m6.971    \u001b[0m | \u001b[0m0.6693   \u001b[0m | \u001b[0m7.557    \u001b[0m | \u001b[0m7.541    \u001b[0m | \u001b[0m9.231    \u001b[0m | \u001b[0m7.118    \u001b[0m | \u001b[0m1.251    \u001b[0m |\n","| \u001b[0m11       \u001b[0m | \u001b[0m-0.5765  \u001b[0m | \u001b[0m1.664    \u001b[0m | \u001b[0m8.981    \u001b[0m | \u001b[0m1.331    \u001b[0m | \u001b[0m8.31     \u001b[0m | \u001b[0m5.356    \u001b[0m | \u001b[0m5.453    \u001b[0m | \u001b[0m0.7578   \u001b[0m | \u001b[0m4.906    \u001b[0m | \u001b[0m8.293    \u001b[0m | \u001b[0m9.432    \u001b[0m | \u001b[0m0.4583   \u001b[0m | \u001b[0m7.868    \u001b[0m |\n","| \u001b[0m12       \u001b[0m | \u001b[0m-0.5222  \u001b[0m | \u001b[0m2.848    \u001b[0m | \u001b[0m9.873    \u001b[0m | \u001b[0m9.896    \u001b[0m | \u001b[0m2.709    \u001b[0m | \u001b[0m9.051    \u001b[0m | \u001b[0m4.605    \u001b[0m | \u001b[0m9.893    \u001b[0m | \u001b[0m7.166    \u001b[0m | \u001b[0m3.921    \u001b[0m | \u001b[0m2.836    \u001b[0m | \u001b[0m8.369    \u001b[0m | \u001b[0m0.2007   \u001b[0m |\n","| \u001b[0m13       \u001b[0m | \u001b[0m-0.2018  \u001b[0m | \u001b[0m8.404    \u001b[0m | \u001b[0m4.807    \u001b[0m | \u001b[0m9.331    \u001b[0m | \u001b[0m9.497    \u001b[0m | \u001b[0m6.444    \u001b[0m | \u001b[0m4.411    \u001b[0m | \u001b[0m8.067    \u001b[0m | \u001b[0m1.068    \u001b[0m | \u001b[0m5.031    \u001b[0m | \u001b[0m7.455    \u001b[0m | \u001b[0m3.411    \u001b[0m | \u001b[0m6.865    \u001b[0m |\n","| \u001b[0m14       \u001b[0m | \u001b[0m-0.1543  \u001b[0m | \u001b[0m5.466    \u001b[0m | \u001b[0m0.1087   \u001b[0m | \u001b[0m5.748    \u001b[0m | \u001b[0m8.177    \u001b[0m | \u001b[0m0.7935   \u001b[0m | \u001b[0m8.739    \u001b[0m | \u001b[0m4.259    \u001b[0m | \u001b[0m5.288    \u001b[0m | \u001b[0m8.572    \u001b[0m | \u001b[0m6.459    \u001b[0m | \u001b[0m3.606    \u001b[0m | \u001b[0m9.221    \u001b[0m |\n","| \u001b[0m15       \u001b[0m | \u001b[0m-0.319   \u001b[0m | \u001b[0m1.406    \u001b[0m | \u001b[0m3.36     \u001b[0m | \u001b[0m5.134    \u001b[0m | \u001b[0m9.644    \u001b[0m | \u001b[0m3.372    \u001b[0m | \u001b[0m2.263    \u001b[0m | \u001b[0m2.021    \u001b[0m | \u001b[0m1.039    \u001b[0m | \u001b[0m4.498    \u001b[0m | \u001b[0m5.877    \u001b[0m | \u001b[0m4.663    \u001b[0m | \u001b[0m3.065    \u001b[0m |\n","| \u001b[0m16       \u001b[0m | \u001b[0m-0.4891  \u001b[0m | \u001b[0m0.9093   \u001b[0m | \u001b[0m3.397    \u001b[0m | \u001b[0m5.779    \u001b[0m | \u001b[0m2.486    \u001b[0m | \u001b[0m6.957    \u001b[0m | \u001b[0m9.819    \u001b[0m | \u001b[0m3.568    \u001b[0m | \u001b[0m3.631    \u001b[0m | \u001b[0m0.9371   \u001b[0m | \u001b[0m2.441    \u001b[0m | \u001b[0m8.094    \u001b[0m | \u001b[0m7.216    \u001b[0m |\n","| \u001b[0m17       \u001b[0m | \u001b[0m-0.4541  \u001b[0m | \u001b[0m2.809    \u001b[0m | \u001b[0m0.9707   \u001b[0m | \u001b[0m9.879    \u001b[0m | \u001b[0m3.772    \u001b[0m | \u001b[0m8.607    \u001b[0m | \u001b[0m1.768    \u001b[0m | \u001b[0m4.141    \u001b[0m | \u001b[0m5.299    \u001b[0m | \u001b[0m8.512    \u001b[0m | \u001b[0m1.607    \u001b[0m | \u001b[0m9.321    \u001b[0m | \u001b[0m8.832    \u001b[0m |\n","| \u001b[0m18       \u001b[0m | \u001b[0m-0.2821  \u001b[0m | \u001b[0m7.569    \u001b[0m | \u001b[0m0.1315   \u001b[0m | \u001b[0m9.51     \u001b[0m | \u001b[0m9.06     \u001b[0m | \u001b[0m5.142    \u001b[0m | \u001b[0m0.3364   \u001b[0m | \u001b[0m1.536    \u001b[0m | \u001b[0m3.225    \u001b[0m | \u001b[0m2.943    \u001b[0m | \u001b[0m3.269    \u001b[0m | \u001b[0m1.507    \u001b[0m | \u001b[0m5.696    \u001b[0m |\n","| \u001b[0m19       \u001b[0m | \u001b[0m-0.4003  \u001b[0m | \u001b[0m4.935    \u001b[0m | \u001b[0m2.412    \u001b[0m | \u001b[0m8.642    \u001b[0m | \u001b[0m4.169    \u001b[0m | \u001b[0m6.458    \u001b[0m | \u001b[0m3.491    \u001b[0m | \u001b[0m9.186    \u001b[0m | \u001b[0m1.713    \u001b[0m | \u001b[0m0.77     \u001b[0m | \u001b[0m2.603    \u001b[0m | \u001b[0m2.796    \u001b[0m | \u001b[0m3.888    \u001b[0m |\n","| \u001b[0m20       \u001b[0m | \u001b[0m-0.6928  \u001b[0m | \u001b[0m7.837    \u001b[0m | \u001b[0m2.45     \u001b[0m | \u001b[0m6.948    \u001b[0m | \u001b[0m7.45     \u001b[0m | \u001b[0m4.47     \u001b[0m | \u001b[0m9.247    \u001b[0m | \u001b[0m1.119    \u001b[0m | \u001b[0m6.561    \u001b[0m | \u001b[0m5.636    \u001b[0m | \u001b[0m1.108    \u001b[0m | \u001b[0m2.861    \u001b[0m | \u001b[0m5.31     \u001b[0m |\n","| \u001b[0m21       \u001b[0m | \u001b[0m-0.701   \u001b[0m | \u001b[0m1.601    \u001b[0m | \u001b[0m8.653    \u001b[0m | \u001b[0m4.721    \u001b[0m | \u001b[0m8.478    \u001b[0m | \u001b[0m7.369    \u001b[0m | \u001b[0m4.253    \u001b[0m | \u001b[0m4.333    \u001b[0m | \u001b[0m6.917    \u001b[0m | \u001b[0m5.925    \u001b[0m | \u001b[0m8.313    \u001b[0m | \u001b[0m8.968    \u001b[0m | \u001b[0m0.06567  \u001b[0m |\n","| \u001b[0m22       \u001b[0m | \u001b[0m-0.2931  \u001b[0m | \u001b[0m3.21     \u001b[0m | \u001b[0m0.597    \u001b[0m | \u001b[0m1.856    \u001b[0m | \u001b[0m8.452    \u001b[0m | \u001b[0m1.337    \u001b[0m | \u001b[0m4.833    \u001b[0m | \u001b[0m3.24     \u001b[0m | \u001b[0m5.297    \u001b[0m | \u001b[0m1.953    \u001b[0m | \u001b[0m3.166    \u001b[0m | \u001b[0m2.083    \u001b[0m | \u001b[0m9.602    \u001b[0m |\n","| \u001b[0m23       \u001b[0m | \u001b[0m-0.6105  \u001b[0m | \u001b[0m4.268    \u001b[0m | \u001b[0m9.109    \u001b[0m | \u001b[0m2.238    \u001b[0m | \u001b[0m0.2207   \u001b[0m | \u001b[0m3.818    \u001b[0m | \u001b[0m2.845    \u001b[0m | \u001b[0m3.616    \u001b[0m | \u001b[0m8.543    \u001b[0m | \u001b[0m0.1748   \u001b[0m | \u001b[0m1.357    \u001b[0m | \u001b[0m5.227    \u001b[0m | \u001b[0m7.883    \u001b[0m |\n","| \u001b[0m24       \u001b[0m | \u001b[0m-0.666   \u001b[0m | \u001b[0m9.476    \u001b[0m | \u001b[0m7.554    \u001b[0m | \u001b[0m3.387    \u001b[0m | \u001b[0m0.2296   \u001b[0m | \u001b[0m2.268    \u001b[0m | \u001b[0m8.703    \u001b[0m | \u001b[0m6.276    \u001b[0m | \u001b[0m1.437    \u001b[0m | \u001b[0m6.287    \u001b[0m | \u001b[0m5.08     \u001b[0m | \u001b[0m9.093    \u001b[0m | \u001b[0m9.11     \u001b[0m |\n","| \u001b[0m25       \u001b[0m | \u001b[0m-0.5657  \u001b[0m | \u001b[0m3.696    \u001b[0m | \u001b[0m4.207    \u001b[0m | \u001b[0m6.929    \u001b[0m | \u001b[0m0.8605   \u001b[0m | \u001b[0m0.9018   \u001b[0m | \u001b[0m7.649    \u001b[0m | \u001b[0m8.295    \u001b[0m | \u001b[0m0.5478   \u001b[0m | \u001b[0m5.806    \u001b[0m | \u001b[0m6.993    \u001b[0m | \u001b[0m5.374    \u001b[0m | \u001b[0m0.2782   \u001b[0m |\n","| \u001b[0m26       \u001b[0m | \u001b[0m-0.7467  \u001b[0m | \u001b[0m7.525    \u001b[0m | \u001b[0m9.018    \u001b[0m | \u001b[0m9.205    \u001b[0m | \u001b[0m4.212    \u001b[0m | \u001b[0m0.8174   \u001b[0m | \u001b[0m7.975    \u001b[0m | \u001b[0m6.998    \u001b[0m | \u001b[0m2.538    \u001b[0m | \u001b[0m5.382    \u001b[0m | \u001b[0m8.532    \u001b[0m | \u001b[0m3.792    \u001b[0m | \u001b[0m1.025    \u001b[0m |\n","| \u001b[0m27       \u001b[0m | \u001b[0m-0.4448  \u001b[0m | \u001b[0m2.804    \u001b[0m | \u001b[0m5.874    \u001b[0m | \u001b[0m3.181    \u001b[0m | \u001b[0m3.114    \u001b[0m | \u001b[0m5.817    \u001b[0m | \u001b[0m3.144    \u001b[0m | \u001b[0m7.44     \u001b[0m | \u001b[0m5.179    \u001b[0m | \u001b[0m9.353    \u001b[0m | \u001b[0m0.2051   \u001b[0m | \u001b[0m4.731    \u001b[0m | \u001b[0m6.147    \u001b[0m |\n","| \u001b[0m28       \u001b[0m | \u001b[0m-0.6415  \u001b[0m | \u001b[0m4.328    \u001b[0m | \u001b[0m9.403    \u001b[0m | \u001b[0m6.374    \u001b[0m | \u001b[0m7.362    \u001b[0m | \u001b[0m7.533    \u001b[0m | \u001b[0m4.841    \u001b[0m | \u001b[0m8.285    \u001b[0m | \u001b[0m0.6593   \u001b[0m | \u001b[0m7.056    \u001b[0m | \u001b[0m5.722    \u001b[0m | \u001b[0m1.464    \u001b[0m | \u001b[0m0.3178   \u001b[0m |\n","| \u001b[0m29       \u001b[0m | \u001b[0m-0.6023  \u001b[0m | \u001b[0m1.437    \u001b[0m | \u001b[0m6.031    \u001b[0m | \u001b[0m1.254    \u001b[0m | \u001b[0m1.099    \u001b[0m | \u001b[0m3.587    \u001b[0m | \u001b[0m8.194    \u001b[0m | \u001b[0m6.224    \u001b[0m | \u001b[0m6.125    \u001b[0m | \u001b[0m3.756    \u001b[0m | \u001b[0m9.334    \u001b[0m | \u001b[0m7.401    \u001b[0m | \u001b[0m9.262    \u001b[0m |\n","| \u001b[0m30       \u001b[0m | \u001b[0m-0.4115  \u001b[0m | \u001b[0m9.498    \u001b[0m | \u001b[0m1.241    \u001b[0m | \u001b[0m5.71     \u001b[0m | \u001b[0m9.724    \u001b[0m | \u001b[0m8.113    \u001b[0m | \u001b[0m3.534    \u001b[0m | \u001b[0m7.351    \u001b[0m | \u001b[0m1.23     \u001b[0m | \u001b[0m9.279    \u001b[0m | \u001b[0m9.281    \u001b[0m | \u001b[0m1.828    \u001b[0m | \u001b[0m4.919    \u001b[0m |\n","| \u001b[0m31       \u001b[0m | \u001b[0m-0.166   \u001b[0m | \u001b[0m7.284    \u001b[0m | \u001b[0m4.831    \u001b[0m | \u001b[0m0.03937  \u001b[0m | \u001b[0m1.902    \u001b[0m | \u001b[0m5.28     \u001b[0m | \u001b[0m5.534    \u001b[0m | \u001b[0m1.942    \u001b[0m | \u001b[0m7.296    \u001b[0m | \u001b[0m2.495    \u001b[0m | \u001b[0m9.12     \u001b[0m | \u001b[0m4.833    \u001b[0m | \u001b[0m6.775    \u001b[0m |\n","| \u001b[0m32       \u001b[0m | \u001b[0m-0.6949  \u001b[0m | \u001b[0m2.176    \u001b[0m | \u001b[0m1.617    \u001b[0m | \u001b[0m4.832    \u001b[0m | \u001b[0m6.054    \u001b[0m | \u001b[0m9.146    \u001b[0m | \u001b[0m4.93     \u001b[0m | \u001b[0m8.309    \u001b[0m | \u001b[0m1.005    \u001b[0m | \u001b[0m5.146    \u001b[0m | \u001b[0m4.822    \u001b[0m | \u001b[0m3.742    \u001b[0m | \u001b[0m2.259    \u001b[0m |\n","| \u001b[0m33       \u001b[0m | \u001b[0m-0.7365  \u001b[0m | \u001b[0m5.312    \u001b[0m | \u001b[0m1.063    \u001b[0m | \u001b[0m0.8036   \u001b[0m | \u001b[0m8.523    \u001b[0m | \u001b[0m9.822    \u001b[0m | \u001b[0m1.793    \u001b[0m | \u001b[0m0.6145   \u001b[0m | \u001b[0m1.528    \u001b[0m | \u001b[0m9.688    \u001b[0m | \u001b[0m7.731    \u001b[0m | \u001b[0m6.769    \u001b[0m | \u001b[0m7.247    \u001b[0m |\n","| \u001b[0m34       \u001b[0m | \u001b[0m-0.3939  \u001b[0m | \u001b[0m9.875    \u001b[0m | \u001b[0m4.41     \u001b[0m | \u001b[0m3.867    \u001b[0m | \u001b[0m0.4891   \u001b[0m | \u001b[0m3.088    \u001b[0m | \u001b[0m7.783    \u001b[0m | \u001b[0m9.365    \u001b[0m | \u001b[0m2.907    \u001b[0m | \u001b[0m8.979    \u001b[0m | \u001b[0m7.24     \u001b[0m | \u001b[0m5.81     \u001b[0m | \u001b[0m0.2507   \u001b[0m |\n","| \u001b[0m35       \u001b[0m | \u001b[0m-0.5731  \u001b[0m | \u001b[0m9.994    \u001b[0m | \u001b[0m2.299    \u001b[0m | \u001b[0m5.76     \u001b[0m | \u001b[0m5.218    \u001b[0m | \u001b[0m4.89     \u001b[0m | \u001b[0m6.851    \u001b[0m | \u001b[0m8.406    \u001b[0m | \u001b[0m5.194    \u001b[0m | \u001b[0m8.203    \u001b[0m | \u001b[0m3.846    \u001b[0m | \u001b[0m0.6059   \u001b[0m | \u001b[0m1.476    \u001b[0m |\n","| \u001b[0m36       \u001b[0m | \u001b[0m-0.7075  \u001b[0m | \u001b[0m9.764    \u001b[0m | \u001b[0m7.804    \u001b[0m | \u001b[0m0.3877   \u001b[0m | \u001b[0m6.065    \u001b[0m | \u001b[0m1.515    \u001b[0m | \u001b[0m4.052    \u001b[0m | \u001b[0m6.461    \u001b[0m | \u001b[0m5.693    \u001b[0m | \u001b[0m7.263    \u001b[0m | \u001b[0m1.786    \u001b[0m | \u001b[0m4.992    \u001b[0m | \u001b[0m0.8564   \u001b[0m |\n","| \u001b[0m37       \u001b[0m | \u001b[0m-0.6284  \u001b[0m | \u001b[0m4.158    \u001b[0m | \u001b[0m7.562    \u001b[0m | \u001b[0m9.273    \u001b[0m | \u001b[0m0.8375   \u001b[0m | \u001b[0m8.835    \u001b[0m | \u001b[0m2.567    \u001b[0m | \u001b[0m9.73     \u001b[0m | \u001b[0m9.496    \u001b[0m | \u001b[0m4.555    \u001b[0m | \u001b[0m0.2356   \u001b[0m | \u001b[0m0.7938   \u001b[0m | \u001b[0m8.106    \u001b[0m |\n","| \u001b[0m38       \u001b[0m | \u001b[0m-0.5956  \u001b[0m | \u001b[0m7.369    \u001b[0m | \u001b[0m7.915    \u001b[0m | \u001b[0m7.612    \u001b[0m | \u001b[0m0.03622  \u001b[0m | \u001b[0m9.772    \u001b[0m | \u001b[0m5.168    \u001b[0m | \u001b[0m8.117    \u001b[0m | \u001b[0m4.533    \u001b[0m | \u001b[0m3.412    \u001b[0m | \u001b[0m3.93     \u001b[0m | \u001b[0m0.6808   \u001b[0m | \u001b[0m1.936    \u001b[0m |\n","| \u001b[0m39       \u001b[0m | \u001b[0m-0.7771  \u001b[0m | \u001b[0m4.836    \u001b[0m | \u001b[0m6.446    \u001b[0m | \u001b[0m7.889    \u001b[0m | \u001b[0m3.779    \u001b[0m | \u001b[0m3.923    \u001b[0m | \u001b[0m2.622    \u001b[0m | \u001b[0m0.1331   \u001b[0m | \u001b[0m9.285    \u001b[0m | \u001b[0m9.034    \u001b[0m | \u001b[0m1.74     \u001b[0m | \u001b[0m8.695    \u001b[0m | \u001b[0m4.59     \u001b[0m |\n","| \u001b[0m40       \u001b[0m | \u001b[0m-0.226   \u001b[0m | \u001b[0m5.855    \u001b[0m | \u001b[0m3.679    \u001b[0m | \u001b[0m6.427    \u001b[0m | \u001b[0m8.934    \u001b[0m | \u001b[0m6.991    \u001b[0m | \u001b[0m8.503    \u001b[0m | \u001b[0m1.599    \u001b[0m | \u001b[0m8.561    \u001b[0m | \u001b[0m4.96     \u001b[0m | \u001b[0m2.904    \u001b[0m | \u001b[0m0.1811   \u001b[0m | \u001b[0m6.837    \u001b[0m |\n","| \u001b[95m41       \u001b[0m | \u001b[95m-0.124   \u001b[0m | \u001b[95m8.82     \u001b[0m | \u001b[95m5.053    \u001b[0m | \u001b[95m7.511    \u001b[0m | \u001b[95m5.312    \u001b[0m | \u001b[95m7.624    \u001b[0m | \u001b[95m0.9447   \u001b[0m | \u001b[95m0.9596   \u001b[0m | \u001b[95m9.486    \u001b[0m | \u001b[95m5.987    \u001b[0m | \u001b[95m5.64     \u001b[0m | \u001b[95m7.14     \u001b[0m | \u001b[95m5.633    \u001b[0m |\n","| \u001b[0m42       \u001b[0m | \u001b[0m-0.5253  \u001b[0m | \u001b[0m3.838    \u001b[0m | \u001b[0m7.883    \u001b[0m | \u001b[0m4.702    \u001b[0m | \u001b[0m9.365    \u001b[0m | \u001b[0m4.024    \u001b[0m | \u001b[0m5.751    \u001b[0m | \u001b[0m4.4      \u001b[0m | \u001b[0m5.444    \u001b[0m | \u001b[0m1.687    \u001b[0m | \u001b[0m4.312    \u001b[0m | \u001b[0m3.562    \u001b[0m | \u001b[0m0.2587   \u001b[0m |\n","| \u001b[0m43       \u001b[0m | \u001b[0m-0.5455  \u001b[0m | \u001b[0m4.622    \u001b[0m | \u001b[0m8.819    \u001b[0m | \u001b[0m7.898    \u001b[0m | \u001b[0m2.03     \u001b[0m | \u001b[0m3.481    \u001b[0m | \u001b[0m0.116    \u001b[0m | \u001b[0m5.145    \u001b[0m | \u001b[0m5.21     \u001b[0m | \u001b[0m7.722    \u001b[0m | \u001b[0m0.9365   \u001b[0m | \u001b[0m7.216    \u001b[0m | \u001b[0m5.381    \u001b[0m |\n","| \u001b[0m44       \u001b[0m | \u001b[0m-0.1794  \u001b[0m | \u001b[0m3.719    \u001b[0m | \u001b[0m8.294    \u001b[0m | \u001b[0m2.589    \u001b[0m | \u001b[0m9.589    \u001b[0m | \u001b[0m7.491    \u001b[0m | \u001b[0m6.181    \u001b[0m | \u001b[0m7.608    \u001b[0m | \u001b[0m4.806    \u001b[0m | \u001b[0m2.335    \u001b[0m | \u001b[0m7.932    \u001b[0m | \u001b[0m1.475    \u001b[0m | \u001b[0m5.471    \u001b[0m |\n","| \u001b[0m45       \u001b[0m | \u001b[0m-0.7125  \u001b[0m | \u001b[0m1.345    \u001b[0m | \u001b[0m9.852    \u001b[0m | \u001b[0m9.916    \u001b[0m | \u001b[0m9.024    \u001b[0m | \u001b[0m1.066    \u001b[0m | \u001b[0m8.26     \u001b[0m | \u001b[0m0.2306   \u001b[0m | \u001b[0m4.923    \u001b[0m | \u001b[0m1.498    \u001b[0m | \u001b[0m5.879    \u001b[0m | \u001b[0m9.594    \u001b[0m | \u001b[0m1.256    \u001b[0m |\n","| \u001b[0m46       \u001b[0m | \u001b[0m-0.4601  \u001b[0m | \u001b[0m2.658    \u001b[0m | \u001b[0m1.247    \u001b[0m | \u001b[0m9.487    \u001b[0m | \u001b[0m3.987    \u001b[0m | \u001b[0m7.944    \u001b[0m | \u001b[0m6.37     \u001b[0m | \u001b[0m8.231    \u001b[0m | \u001b[0m7.214    \u001b[0m | \u001b[0m2.498    \u001b[0m | \u001b[0m9.277    \u001b[0m | \u001b[0m9.476    \u001b[0m | \u001b[0m8.111    \u001b[0m |\n","| \u001b[0m47       \u001b[0m | \u001b[0m-0.2304  \u001b[0m | \u001b[0m2.584    \u001b[0m | \u001b[0m7.539    \u001b[0m | \u001b[0m4.279    \u001b[0m | \u001b[0m9.766    \u001b[0m | \u001b[0m7.558    \u001b[0m | \u001b[0m5.812    \u001b[0m | \u001b[0m1.565    \u001b[0m | \u001b[0m2.749    \u001b[0m | \u001b[0m7.272    \u001b[0m | \u001b[0m6.952    \u001b[0m | \u001b[0m3.577    \u001b[0m | \u001b[0m3.027    \u001b[0m |\n","| \u001b[0m48       \u001b[0m | \u001b[0m-0.4729  \u001b[0m | \u001b[0m0.5015   \u001b[0m | \u001b[0m8.338    \u001b[0m | \u001b[0m7.233    \u001b[0m | \u001b[0m6.55     \u001b[0m | \u001b[0m6.428    \u001b[0m | \u001b[0m5.644    \u001b[0m | \u001b[0m7.401    \u001b[0m | \u001b[0m5.146    \u001b[0m | \u001b[0m4.747    \u001b[0m | \u001b[0m3.34     \u001b[0m | \u001b[0m4.218    \u001b[0m | \u001b[0m5.316    \u001b[0m |\n","| \u001b[0m49       \u001b[0m | \u001b[0m-0.3809  \u001b[0m | \u001b[0m9.472    \u001b[0m | \u001b[0m8.135    \u001b[0m | \u001b[0m1.268    \u001b[0m | \u001b[0m0.7035   \u001b[0m | \u001b[0m9.864    \u001b[0m | \u001b[0m9.215    \u001b[0m | \u001b[0m6.081    \u001b[0m | \u001b[0m6.234    \u001b[0m | \u001b[0m9.71     \u001b[0m | \u001b[0m8.203    \u001b[0m | \u001b[0m5.687    \u001b[0m | \u001b[0m2.203    \u001b[0m |\n","| \u001b[0m50       \u001b[0m | \u001b[0m-0.4558  \u001b[0m | \u001b[0m1.02     \u001b[0m | \u001b[0m0.8207   \u001b[0m | \u001b[0m8.897    \u001b[0m | \u001b[0m1.335    \u001b[0m | \u001b[0m2.414    \u001b[0m | \u001b[0m3.725    \u001b[0m | \u001b[0m4.63     \u001b[0m | \u001b[0m9.84     \u001b[0m | \u001b[0m0.958    \u001b[0m | \u001b[0m6.731    \u001b[0m | \u001b[0m0.1371   \u001b[0m | \u001b[0m9.704    \u001b[0m |\n","| \u001b[0m51       \u001b[0m | \u001b[0m-0.2687  \u001b[0m | \u001b[0m7.851    \u001b[0m | \u001b[0m6.295    \u001b[0m | \u001b[0m1.986    \u001b[0m | \u001b[0m6.715    \u001b[0m | \u001b[0m8.587    \u001b[0m | \u001b[0m6.395    \u001b[0m | \u001b[0m0.9425   \u001b[0m | \u001b[0m4.568    \u001b[0m | \u001b[0m9.014    \u001b[0m | \u001b[0m2.765    \u001b[0m | \u001b[0m6.301    \u001b[0m | \u001b[0m5.678    \u001b[0m |\n","| \u001b[0m52       \u001b[0m | \u001b[0m-0.4804  \u001b[0m | \u001b[0m7.609    \u001b[0m | \u001b[0m4.605    \u001b[0m | \u001b[0m7.161    \u001b[0m | \u001b[0m1.475    \u001b[0m | \u001b[0m0.6735   \u001b[0m | \u001b[0m3.473    \u001b[0m | \u001b[0m4.403    \u001b[0m | \u001b[0m0.8042   \u001b[0m | \u001b[0m0.8449   \u001b[0m | \u001b[0m9.462    \u001b[0m | \u001b[0m9.294    \u001b[0m | \u001b[0m9.806    \u001b[0m |\n","| \u001b[0m53       \u001b[0m | \u001b[0m-0.3422  \u001b[0m | \u001b[0m6.04     \u001b[0m | \u001b[0m7.256    \u001b[0m | \u001b[0m4.522    \u001b[0m | \u001b[0m7.081    \u001b[0m | \u001b[0m0.06179  \u001b[0m | \u001b[0m1.818    \u001b[0m | \u001b[0m1.666    \u001b[0m | \u001b[0m6.506    \u001b[0m | \u001b[0m5.471    \u001b[0m | \u001b[0m9.447    \u001b[0m | \u001b[0m8.304    \u001b[0m | \u001b[0m3.532    \u001b[0m |\n","| \u001b[0m54       \u001b[0m | \u001b[0m-0.4507  \u001b[0m | \u001b[0m0.04164  \u001b[0m | \u001b[0m0.6594   \u001b[0m | \u001b[0m9.204    \u001b[0m | \u001b[0m8.965    \u001b[0m | \u001b[0m0.6349   \u001b[0m | \u001b[0m3.915    \u001b[0m | \u001b[0m6.768    \u001b[0m | \u001b[0m9.301    \u001b[0m | \u001b[0m9.651    \u001b[0m | \u001b[0m7.505    \u001b[0m | \u001b[0m3.81     \u001b[0m | \u001b[0m3.493    \u001b[0m |\n","| \u001b[0m55       \u001b[0m | \u001b[0m-0.589   \u001b[0m | \u001b[0m5.493    \u001b[0m | \u001b[0m7.315    \u001b[0m | \u001b[0m3.406    \u001b[0m | \u001b[0m6.576    \u001b[0m | \u001b[0m7.169    \u001b[0m | \u001b[0m0.08916  \u001b[0m | \u001b[0m7.649    \u001b[0m | \u001b[0m3.798    \u001b[0m | \u001b[0m1.767    \u001b[0m | \u001b[0m9.744    \u001b[0m | \u001b[0m9.35     \u001b[0m | \u001b[0m6.079    \u001b[0m |\n","| \u001b[0m56       \u001b[0m | \u001b[0m-0.2945  \u001b[0m | \u001b[0m2.192    \u001b[0m | \u001b[0m4.812    \u001b[0m | \u001b[0m0.8695   \u001b[0m | \u001b[0m0.2903   \u001b[0m | \u001b[0m8.428    \u001b[0m | \u001b[0m6.969    \u001b[0m | \u001b[0m4.057    \u001b[0m | \u001b[0m3.068    \u001b[0m | \u001b[0m8.055    \u001b[0m | \u001b[0m0.5149   \u001b[0m | \u001b[0m9.342    \u001b[0m | \u001b[0m6.569    \u001b[0m |\n","| \u001b[0m57       \u001b[0m | \u001b[0m-0.6447  \u001b[0m | \u001b[0m3.517    \u001b[0m | \u001b[0m6.394    \u001b[0m | \u001b[0m5.289    \u001b[0m | \u001b[0m3.072    \u001b[0m | \u001b[0m7.244    \u001b[0m | \u001b[0m2.692    \u001b[0m | \u001b[0m0.4611   \u001b[0m | \u001b[0m2.299    \u001b[0m | \u001b[0m0.05201  \u001b[0m | \u001b[0m5.77     \u001b[0m | \u001b[0m3.757    \u001b[0m | \u001b[0m1.727    \u001b[0m |\n","| \u001b[0m58       \u001b[0m | \u001b[0m-0.5774  \u001b[0m | \u001b[0m1.23     \u001b[0m | \u001b[0m9.77     \u001b[0m | \u001b[0m4.817    \u001b[0m | \u001b[0m2.793    \u001b[0m | \u001b[0m2.401    \u001b[0m | \u001b[0m6.579    \u001b[0m | \u001b[0m9.978    \u001b[0m | \u001b[0m8.667    \u001b[0m | \u001b[0m7.497    \u001b[0m | \u001b[0m2.775    \u001b[0m | \u001b[0m7.295    \u001b[0m | \u001b[0m4.211    \u001b[0m |\n","| \u001b[0m59       \u001b[0m | \u001b[0m-0.4019  \u001b[0m | \u001b[0m5.994    \u001b[0m | \u001b[0m3.534    \u001b[0m | \u001b[0m0.1891   \u001b[0m | \u001b[0m9.27     \u001b[0m | \u001b[0m6.576    \u001b[0m | \u001b[0m3.639    \u001b[0m | \u001b[0m8.824    \u001b[0m | \u001b[0m1.971    \u001b[0m | \u001b[0m7.015    \u001b[0m | \u001b[0m1.66     \u001b[0m | \u001b[0m2.58     \u001b[0m | \u001b[0m2.801    \u001b[0m |\n","| \u001b[0m60       \u001b[0m | \u001b[0m-0.199   \u001b[0m | \u001b[0m5.506    \u001b[0m | \u001b[0m1.524    \u001b[0m | \u001b[0m2.974    \u001b[0m | \u001b[0m3.191    \u001b[0m | \u001b[0m2.74     \u001b[0m | \u001b[0m9.963    \u001b[0m | \u001b[0m0.8559   \u001b[0m | \u001b[0m6.293    \u001b[0m | \u001b[0m4.298    \u001b[0m | \u001b[0m9.779    \u001b[0m | \u001b[0m1.197    \u001b[0m | \u001b[0m9.219    \u001b[0m |\n","=========================================================================================================================================================================\n","Optimal scalar values: [8.819941663916048, 5.053426899220153, 7.6239443377952645, 0.9446804763115486, 0.9596069028966052, 9.485856712388017, 5.986795648201071, 5.640246475612508, 7.140375069375786, 5.633454530947568, 7.51139312566652, 5.312489909763755]\n","Minimum difference between probabilities of 'he' and 'she': 0.12401361018419266\n"]}],"source":["!pip install bayesian-optimization\n","\n","import torch\n","from transformers import BertTokenizer, BertForMaskedLM\n","from bayes_opt import BayesianOptimization\n","\n","def modify_attention_heads(model, scalar_values):\n","    model_dict = model.state_dict()\n","\n","    attention_keys = [key for key in model_dict.keys() if 'attention.self.query.weight' in key]\n","\n","    for key, scalar in zip(attention_keys, scalar_values):\n","        layer_num = key.split('bert.encoder.layer.')[1].split('.')[0]\n","        model_dict[f'bert.encoder.layer.{layer_num}.attention.self.query.weight'] *= scalar\n","\n","    model.load_state_dict(model_dict)\n","    return model\n","\n","def get_masked_token_probabilities(sentence, mask_words_list, scalar_values):\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","    model = modify_attention_heads(model, scalar_values)\n","\n","    inputs = tokenizer(sentence, return_tensors=\"pt\")\n","    mask_token_index = torch.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)[0]\n","\n","    if len(mask_token_index) != 1:\n","        raise ValueError(\"Please provide a sentence with exactly one [MASK] token.\")\n","\n","    mask_token_index = mask_token_index.item()\n","\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    logits = outputs.logits\n","    mask_token_logits = logits[0, mask_token_index, :]\n","\n","    probabilities = torch.softmax(mask_token_logits, dim=0)\n","    word_probabilities = {word: probabilities[tokenizer.convert_tokens_to_ids(word)].item() for word in mask_words_list}\n","\n","    return word_probabilities['he'] - word_probabilities['she']\n","\n","def optimize_scalar_values(sentence, mask_words_list):\n","    def objective(**scalar_values):\n","        scalar_values = [scalar_values[f'scalar_{i}'] for i in range(12)]\n","        return -get_masked_token_probabilities(sentence, mask_words_list, scalar_values)  # Negative because we are minimizing\n","\n","    # Define bounds for scalar values\n","    bounds = {'scalar_' + str(i): (0.01, 10.0) for i in range(12)}\n","\n","    optimizer = BayesianOptimization(\n","        f=objective,\n","        pbounds=bounds,\n","        verbose=2,\n","        random_state=1,\n","    )\n","\n","    optimizer.maximize(init_points=10, n_iter=50)\n","\n","    optimal_scalar_values = [optimizer.max['params'][f'scalar_{i}'] for i in range(12)]\n","    return optimal_scalar_values, -optimizer.max['target']\n","\n","# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Find optimal scalar values\n","optimal_scalar_values, min_difference = optimize_scalar_values(sentence, mask_words_list)\n","\n","print(f\"Optimal scalar values: {optimal_scalar_values}\")\n","print(f\"Minimum difference between probabilities of 'he' and 'she': {min_difference}\")\n"]},{"cell_type":"markdown","metadata":{"id":"9kCT-oirLJJ1"},"source":["**Check**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":251,"referenced_widgets":["5f3742d864a34e7ea18dcf9ad1cfc8cc","7fd127e8d1244a92878a1dab811dacf1","7a664ecc69794d08b3fb79c414ae9561","502121ebacec4bb2ba5a13d2036605ef","0ead6ddc6b554ec19b3d5aa1a7d1fe53","97931a7feb5a4df5acf5010ac6e278c1","439e549563274697babd05c4289defaa","52d33ef4ecc04095b4656f5dacf1c008","59618e8dc2684a89a7f8eb4ce95d0233","818cb38d70034f939d5465d08eb3e0bd","14b06d8a3ff742b398bdb4177378ba97"]},"executionInfo":{"elapsed":322053,"status":"ok","timestamp":1713680353202,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"ZK-2nZIsEgyn","outputId":"68b5d156-082c-40f5-d003-deeae6433126"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f3742d864a34e7ea18dcf9ad1cfc8cc","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Probabilities of specified words:\n","he: 0.14582973718643188\n","she: 0.01995725929737091\n","Mounted at /content/drive\n","-----------------------------------------------\n","Beta = [8.82, 5.05, 7.62, 0.94, 0.96, 9.49, 5.99, 5.64, 7.14, 5.63, 7.51, 5.31]\n","Winogender - Average gender bias in bert:  0.61\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["\n","\n","# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [8.82, 5.05, 7.62, 0.94, 0.96, 9.49, 5.99, 5.64, 7.14, 5.63, 7.51, 5.31]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = [8.82, 5.05, 7.62, 0.94, 0.96, 9.49, 5.99, 5.64, 7.14, 5.63, 7.51, 5.31]')\n","print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"markdown","metadata":{"id":"XiYd9uKXVyHx"},"source":["# ***Bayesian Optimization for Entire Dataset***"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16928139,"status":"ok","timestamp":1713702022941,"user":{"displayName":"Shweta","userId":"05242245977803223502"},"user_tz":300},"id":"gnu0ZN4gSVfg","outputId":"2440ca58-eaf6-402e-8229-cae6d5eb08c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting bayesian-optimization\n","  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.25.2)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n","Collecting colorama>=0.4.6 (from bayesian-optimization)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.4.0)\n","Installing collected packages: colorama, bayesian-optimization\n","Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n","|   iter    |  target   | scalar_0  | scalar_1  | scalar_10 | scalar_11 | scalar_2  | scalar_3  | scalar_4  | scalar_5  | scalar_6  | scalar_7  | scalar_8  | scalar_9  |\n","-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","| \u001b[0m1        \u001b[0m | \u001b[0m0.6562   \u001b[0m | \u001b[0m4.176    \u001b[0m | \u001b[0m7.206    \u001b[0m | \u001b[0m0.01114  \u001b[0m | \u001b[0m3.03     \u001b[0m | \u001b[0m1.476    \u001b[0m | \u001b[0m0.9325   \u001b[0m | \u001b[0m1.871    \u001b[0m | \u001b[0m3.462    \u001b[0m | \u001b[0m3.974    \u001b[0m | \u001b[0m5.393    \u001b[0m | \u001b[0m4.198    \u001b[0m | \u001b[0m6.855    \u001b[0m |\n","| \u001b[0m2        \u001b[0m | \u001b[0m0.6008   \u001b[0m | \u001b[0m2.052    \u001b[0m | \u001b[0m8.782    \u001b[0m | \u001b[0m0.2836   \u001b[0m | \u001b[0m6.708    \u001b[0m | \u001b[0m4.179    \u001b[0m | \u001b[0m5.591    \u001b[0m | \u001b[0m1.412    \u001b[0m | \u001b[0m1.989    \u001b[0m | \u001b[0m8.009    \u001b[0m | \u001b[0m9.683    \u001b[0m | \u001b[0m3.141    \u001b[0m | \u001b[0m6.926    \u001b[0m |\n","| \u001b[0m3        \u001b[0m | \u001b[0m0.6217   \u001b[0m | \u001b[0m8.765    \u001b[0m | \u001b[0m8.947    \u001b[0m | \u001b[0m0.8596   \u001b[0m | \u001b[0m0.4002   \u001b[0m | \u001b[0m1.707    \u001b[0m | \u001b[0m8.783    \u001b[0m | \u001b[0m0.9925   \u001b[0m | \u001b[0m4.217    \u001b[0m | \u001b[0m9.579    \u001b[0m | \u001b[0m5.336    \u001b[0m | \u001b[0m6.922    \u001b[0m | \u001b[0m3.162    \u001b[0m |\n","| \u001b[95m4        \u001b[0m | \u001b[95m0.6641   \u001b[0m | \u001b[95m6.868    \u001b[0m | \u001b[95m8.348    \u001b[0m | \u001b[95m0.1927   \u001b[0m | \u001b[95m7.504    \u001b[0m | \u001b[95m9.889    \u001b[0m | \u001b[95m7.484    \u001b[0m | \u001b[95m2.812    \u001b[0m | \u001b[95m7.895    \u001b[0m | \u001b[95m1.041    \u001b[0m | \u001b[95m4.484    \u001b[0m | \u001b[95m9.087    \u001b[0m | \u001b[95m2.943    \u001b[0m |\n","| \u001b[95m5        \u001b[0m | \u001b[95m0.6673   \u001b[0m | \u001b[95m2.885    \u001b[0m | \u001b[95m1.309    \u001b[0m | \u001b[95m0.2035   \u001b[0m | \u001b[95m6.792    \u001b[0m | \u001b[95m2.124    \u001b[0m | \u001b[95m2.663    \u001b[0m | \u001b[95m4.921    \u001b[0m | \u001b[95m0.5431   \u001b[0m | \u001b[95m5.745    \u001b[0m | \u001b[95m1.476    \u001b[0m | \u001b[95m5.897    \u001b[0m | \u001b[95m7.001    \u001b[0m |\n","| \u001b[0m6        \u001b[0m | \u001b[0m0.6538   \u001b[0m | \u001b[0m1.032    \u001b[0m | \u001b[0m4.146    \u001b[0m | \u001b[0m6.947    \u001b[0m | \u001b[0m4.148    \u001b[0m | \u001b[0m0.509    \u001b[0m | \u001b[0m5.364    \u001b[0m | \u001b[0m6.641    \u001b[0m | \u001b[0m5.154    \u001b[0m | \u001b[0m9.447    \u001b[0m | \u001b[0m5.87     \u001b[0m | \u001b[0m9.035    \u001b[0m | \u001b[0m1.383    \u001b[0m |\n","| \u001b[0m7        \u001b[0m | \u001b[0m0.5959   \u001b[0m | \u001b[0m1.401    \u001b[0m | \u001b[0m8.076    \u001b[0m | \u001b[0m3.983    \u001b[0m | \u001b[0m1.662    \u001b[0m | \u001b[0m9.276    \u001b[0m | \u001b[0m3.484    \u001b[0m | \u001b[0m7.511    \u001b[0m | \u001b[0m7.263    \u001b[0m | \u001b[0m8.834    \u001b[0m | \u001b[0m6.24     \u001b[0m | \u001b[0m7.512    \u001b[0m | \u001b[0m3.495    \u001b[0m |\n","| \u001b[0m8        \u001b[0m | \u001b[0m0.5862   \u001b[0m | \u001b[0m2.707    \u001b[0m | \u001b[0m8.96     \u001b[0m | \u001b[0m4.287    \u001b[0m | \u001b[0m9.649    \u001b[0m | \u001b[0m6.638    \u001b[0m | \u001b[0m6.221    \u001b[0m | \u001b[0m1.156    \u001b[0m | \u001b[0m9.495    \u001b[0m | \u001b[0m4.505    \u001b[0m | \u001b[0m5.788    \u001b[0m | \u001b[0m4.087    \u001b[0m | \u001b[0m2.378    \u001b[0m |\n","| \u001b[0m9        \u001b[0m | \u001b[0m0.5697   \u001b[0m | \u001b[0m9.035    \u001b[0m | \u001b[0m5.741    \u001b[0m | \u001b[0m0.03867  \u001b[0m | \u001b[0m6.175    \u001b[0m | \u001b[0m3.273    \u001b[0m | \u001b[0m5.275    \u001b[0m | \u001b[0m8.861    \u001b[0m | \u001b[0m3.579    \u001b[0m | \u001b[0m9.086    \u001b[0m | \u001b[0m6.237    \u001b[0m | \u001b[0m0.1681   \u001b[0m | \u001b[0m9.295    \u001b[0m |\n","| \u001b[0m10       \u001b[0m | \u001b[0m0.6285   \u001b[0m | \u001b[0m6.912    \u001b[0m | \u001b[0m9.973    \u001b[0m | \u001b[0m1.732    \u001b[0m | \u001b[0m1.38     \u001b[0m | \u001b[0m9.327    \u001b[0m | \u001b[0m6.971    \u001b[0m | \u001b[0m0.6693   \u001b[0m | \u001b[0m7.557    \u001b[0m | \u001b[0m7.541    \u001b[0m | \u001b[0m9.231    \u001b[0m | \u001b[0m7.118    \u001b[0m | \u001b[0m1.251    \u001b[0m |\n","| \u001b[0m11       \u001b[0m | \u001b[0m0.6443   \u001b[0m | \u001b[0m2.588    \u001b[0m | \u001b[0m4.179    \u001b[0m | \u001b[0m2.486    \u001b[0m | \u001b[0m9.637    \u001b[0m | \u001b[0m1.675    \u001b[0m | \u001b[0m6.734    \u001b[0m | \u001b[0m7.347    \u001b[0m | \u001b[0m5.867    \u001b[0m | \u001b[0m8.115    \u001b[0m | \u001b[0m2.592    \u001b[0m | \u001b[0m4.127    \u001b[0m | \u001b[0m5.981    \u001b[0m |\n","| \u001b[0m12       \u001b[0m | \u001b[0m0.6495   \u001b[0m | \u001b[0m1.354    \u001b[0m | \u001b[0m5.754    \u001b[0m | \u001b[0m7.246    \u001b[0m | \u001b[0m4.633    \u001b[0m | \u001b[0m0.5461   \u001b[0m | \u001b[0m6.369    \u001b[0m | \u001b[0m6.617    \u001b[0m | \u001b[0m5.082    \u001b[0m | \u001b[0m7.521    \u001b[0m | \u001b[0m3.728    \u001b[0m | \u001b[0m8.336    \u001b[0m | \u001b[0m1.326    \u001b[0m |\n","| \u001b[0m13       \u001b[0m | \u001b[0m0.6563   \u001b[0m | \u001b[0m2.574    \u001b[0m | \u001b[0m3.6      \u001b[0m | \u001b[0m1.68     \u001b[0m | \u001b[0m4.854    \u001b[0m | \u001b[0m0.2519   \u001b[0m | \u001b[0m2.183    \u001b[0m | \u001b[0m4.063    \u001b[0m | \u001b[0m2.896    \u001b[0m | \u001b[0m4.855    \u001b[0m | \u001b[0m2.582    \u001b[0m | \u001b[0m6.609    \u001b[0m | \u001b[0m5.491    \u001b[0m |\n","| \u001b[0m14       \u001b[0m | \u001b[0m0.6561   \u001b[0m | \u001b[0m5.35     \u001b[0m | \u001b[0m3.158    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.92     \u001b[0m | \u001b[0m5.36     \u001b[0m | \u001b[0m4.147    \u001b[0m | \u001b[0m3.415    \u001b[0m | \u001b[0m2.54     \u001b[0m | \u001b[0m1.168    \u001b[0m | \u001b[0m1.083    \u001b[0m | \u001b[0m9.135    \u001b[0m | \u001b[0m5.698    \u001b[0m |\n","| \u001b[0m15       \u001b[0m | \u001b[0m0.6291   \u001b[0m | \u001b[0m0.6752   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.599    \u001b[0m | \u001b[0m9.261    \u001b[0m | \u001b[0m0.4038   \u001b[0m | \u001b[0m5.439    \u001b[0m | \u001b[0m7.145    \u001b[0m | \u001b[0m0.8692   \u001b[0m | \u001b[0m6.885    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m4.162    \u001b[0m |\n","| \u001b[95m16       \u001b[0m | \u001b[95m0.6862   \u001b[0m | \u001b[95m3.547    \u001b[0m | \u001b[95m2.306    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m4.582    \u001b[0m | \u001b[95m2.499    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m1.3      \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m1.884    \u001b[0m | \u001b[95m0.8926   \u001b[0m | \u001b[95m5.113    \u001b[0m | \u001b[95m8.723    \u001b[0m |\n","| \u001b[0m17       \u001b[0m | \u001b[0m0.582    \u001b[0m | \u001b[0m0.06647  \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.388    \u001b[0m | \u001b[0m0.6209   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.935    \u001b[0m | \u001b[0m1.619    \u001b[0m | \u001b[0m0.1637   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.053    \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[95m18       \u001b[0m | \u001b[95m0.6971   \u001b[0m | \u001b[95m4.98     \u001b[0m | \u001b[95m3.401    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m4.519    \u001b[0m | \u001b[95m3.291    \u001b[0m | \u001b[95m0.957    \u001b[0m | \u001b[95m1.888    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m3.805    \u001b[0m | \u001b[95m1.788    \u001b[0m | \u001b[95m6.753    \u001b[0m | \u001b[95m7.367    \u001b[0m |\n","| \u001b[0m19       \u001b[0m | \u001b[0m0.6441   \u001b[0m | \u001b[0m6.008    \u001b[0m | \u001b[0m3.119    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m1.04     \u001b[0m | \u001b[0m2.412    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.699    \u001b[0m | \u001b[0m0.5011   \u001b[0m | \u001b[0m7.93     \u001b[0m | \u001b[0m9.048    \u001b[0m |\n","| \u001b[0m20       \u001b[0m | \u001b[0m0.6573   \u001b[0m | \u001b[0m4.724    \u001b[0m | \u001b[0m3.246    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.994    \u001b[0m | \u001b[0m3.719    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.1415   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m3.481    \u001b[0m | \u001b[0m1.405    \u001b[0m | \u001b[0m5.121    \u001b[0m | \u001b[0m5.812    \u001b[0m |\n","| \u001b[0m21       \u001b[0m | \u001b[0m0.68     \u001b[0m | \u001b[0m3.566    \u001b[0m | \u001b[0m3.643    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m4.766    \u001b[0m | \u001b[0m4.053    \u001b[0m | \u001b[0m0.1236   \u001b[0m | \u001b[0m4.04     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.584    \u001b[0m | \u001b[0m2.927    \u001b[0m | \u001b[0m6.975    \u001b[0m | \u001b[0m8.93     \u001b[0m |\n","| \u001b[0m22       \u001b[0m | \u001b[0m0.6873   \u001b[0m | \u001b[0m3.71     \u001b[0m | \u001b[0m4.087    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m3.5      \u001b[0m | \u001b[0m4.231    \u001b[0m | \u001b[0m2.88     \u001b[0m | \u001b[0m2.853    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m3.668    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m5.059    \u001b[0m | \u001b[0m8.14     \u001b[0m |\n","| \u001b[0m23       \u001b[0m | \u001b[0m0.6553   \u001b[0m | \u001b[0m4.585    \u001b[0m | \u001b[0m0.4763   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.829    \u001b[0m | \u001b[0m4.289    \u001b[0m | \u001b[0m2.434    \u001b[0m | \u001b[0m2.387    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m3.279    \u001b[0m | \u001b[0m3.191    \u001b[0m | \u001b[0m5.347    \u001b[0m | \u001b[0m7.131    \u001b[0m |\n","| \u001b[95m24       \u001b[0m | \u001b[95m0.6992   \u001b[0m | \u001b[95m4.965    \u001b[0m | \u001b[95m4.923    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m6.188    \u001b[0m | \u001b[95m1.653    \u001b[0m | \u001b[95m1.77     \u001b[0m | \u001b[95m2.5      \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m4.128    \u001b[0m | \u001b[95m0.06871  \u001b[0m | \u001b[95m6.978    \u001b[0m | \u001b[95m10.0     \u001b[0m |\n","| \u001b[0m25       \u001b[0m | \u001b[0m0.6897   \u001b[0m | \u001b[0m6.615    \u001b[0m | \u001b[0m5.147    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m4.8      \u001b[0m | \u001b[0m1.331    \u001b[0m | \u001b[0m0.03301  \u001b[0m | \u001b[0m4.738    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.948    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m5.658    \u001b[0m | \u001b[0m7.481    \u001b[0m |\n","| \u001b[0m26       \u001b[0m | \u001b[0m0.6875   \u001b[0m | \u001b[0m3.533    \u001b[0m | \u001b[0m5.184    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m4.252    \u001b[0m | \u001b[0m2.247    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m3.157    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m7.702    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.724    \u001b[0m | \u001b[0m8.958    \u001b[0m |\n","| \u001b[0m27       \u001b[0m | \u001b[0m0.6833   \u001b[0m | \u001b[0m5.826    \u001b[0m | \u001b[0m3.995    \u001b[0m | \u001b[0m4.53     \u001b[0m | \u001b[0m5.634    \u001b[0m | \u001b[0m3.236    \u001b[0m | \u001b[0m0.1739   \u001b[0m | \u001b[0m3.139    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m4.778    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.612    \u001b[0m | \u001b[0m9.726    \u001b[0m |\n","| \u001b[0m28       \u001b[0m | \u001b[0m0.669    \u001b[0m | \u001b[0m3.275    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m1.208    \u001b[0m | \u001b[0m5.315    \u001b[0m | \u001b[0m1.771    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.482    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.535    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m8.314    \u001b[0m | \u001b[0m8.478    \u001b[0m |\n","| \u001b[0m29       \u001b[0m | \u001b[0m0.6908   \u001b[0m | \u001b[0m8.439    \u001b[0m | \u001b[0m2.734    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m7.889    \u001b[0m | \u001b[0m2.066    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m4.435    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.867    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m30       \u001b[0m | \u001b[0m0.6766   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m8.96     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.691    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.552    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[95m31       \u001b[0m | \u001b[95m0.7008   \u001b[0m | \u001b[95m9.255    \u001b[0m | \u001b[95m7.058    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m7.787    \u001b[0m | \u001b[95m7.693    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m4.386    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m5.252    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m10.0     \u001b[0m |\n","| \u001b[0m32       \u001b[0m | \u001b[0m0.6909   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m7.355    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m5.213    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m8.524    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m1.249    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m33       \u001b[0m | \u001b[0m0.5519   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.521    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[95m34       \u001b[0m | \u001b[95m0.702    \u001b[0m | \u001b[95m8.507    \u001b[0m | \u001b[95m6.917    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m6.715    \u001b[0m | \u001b[95m5.143    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m8.796    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m6.739    \u001b[0m | \u001b[95m0.01     \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m10.0     \u001b[0m |\n","| \u001b[0m35       \u001b[0m | \u001b[0m0.6974   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m2.953    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m8.119    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m7.984    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.825    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m36       \u001b[0m | \u001b[0m0.6983   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m5.875    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m7.988    \u001b[0m | \u001b[0m5.705    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.278    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m7.143    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m5.142    \u001b[0m |\n","| \u001b[0m37       \u001b[0m | \u001b[0m0.6993   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m4.735    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m5.026    \u001b[0m | \u001b[0m8.513    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m4.262    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m38       \u001b[0m | \u001b[0m0.6845   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m5.561    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m7.801    \u001b[0m | \u001b[0m5.947    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m5.919    \u001b[0m | \u001b[0m5.333    \u001b[0m | \u001b[0m7.185    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m39       \u001b[0m | \u001b[0m0.6471   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m7.054    \u001b[0m | \u001b[0m4.669    \u001b[0m | \u001b[0m9.722    \u001b[0m | \u001b[0m6.634    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.009    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m40       \u001b[0m | \u001b[0m0.5239   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m4.141    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m |\n","| \u001b[0m41       \u001b[0m | \u001b[0m0.6988   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m2.853    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m3.92     \u001b[0m | \u001b[0m7.804    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m7.945    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m4.093    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m9.318    \u001b[0m |\n","| \u001b[0m42       \u001b[0m | \u001b[0m0.5148   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m |\n","| \u001b[0m43       \u001b[0m | \u001b[0m0.5537   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m44       \u001b[0m | \u001b[0m0.6588   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m4.261    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.5005   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m45       \u001b[0m | \u001b[0m0.4898   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m46       \u001b[0m | \u001b[0m0.6601   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m47       \u001b[0m | \u001b[0m0.6379   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m |\n","| \u001b[0m48       \u001b[0m | \u001b[0m0.6115   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m |\n","| \u001b[0m49       \u001b[0m | \u001b[0m0.6023   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m |\n","| \u001b[0m50       \u001b[0m | \u001b[0m0.6519   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m9.989    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m2.513    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m51       \u001b[0m | \u001b[0m0.6667   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.852    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m8.211    \u001b[0m | \u001b[0m2.498    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m52       \u001b[0m | \u001b[0m0.5666   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m |\n","| \u001b[0m53       \u001b[0m | \u001b[0m0.637    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m3.339    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m |\n","| \u001b[0m54       \u001b[0m | \u001b[0m0.6646   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m55       \u001b[0m | \u001b[0m0.6848   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m4.508    \u001b[0m | \u001b[0m8.572    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m56       \u001b[0m | \u001b[0m0.6829   \u001b[0m | \u001b[0m3.615    \u001b[0m | \u001b[0m2.012    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.512    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m5.352    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m8.547    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m57       \u001b[0m | \u001b[0m0.655    \u001b[0m | \u001b[0m3.656    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m4.613    \u001b[0m | \u001b[0m4.103    \u001b[0m | \u001b[0m3.046    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m58       \u001b[0m | \u001b[0m0.6278   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m4.418    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.7696   \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m59       \u001b[0m | \u001b[0m0.6568   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m5.272    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.299    \u001b[0m | \u001b[0m5.941    \u001b[0m | \u001b[0m4.799    \u001b[0m | \u001b[0m5.951    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m5.951    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m10.0     \u001b[0m |\n","| \u001b[0m60       \u001b[0m | \u001b[0m0.6485   \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m6.896    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m1.26     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m4.97     \u001b[0m |\n","=========================================================================================================================================================================\n","Optimal scalar values: [8.506785689205023, 6.917053160948551, 5.142978261322202, 0.01, 8.7963702902574, 0.01, 6.738514906712526, 0.01, 10.0, 10.0, 0.01, 6.715197517842472]\n","Minimum average gender bias in Winogender dataset: 0.702\n"]}],"source":["!pip install bayesian-optimization\n","\n","import torch\n","from transformers import BertTokenizer, BertForMaskedLM\n","from bayes_opt import BayesianOptimization\n","import pandas as pd\n","\n","def modify_attention_heads(model, scalar_values):\n","    model_dict = model.state_dict()\n","\n","    attention_keys = [key for key in model_dict.keys() if 'attention.self.query.weight' in key]\n","\n","    for key, scalar in zip(attention_keys, scalar_values):\n","        layer_num = key.split('bert.encoder.layer.')[1].split('.')[0]\n","        model_dict[f'bert.encoder.layer.{layer_num}.attention.self.query.weight'] *= scalar\n","\n","    model.load_state_dict(model_dict)\n","    return model\n","\n","def get_masked_token_probabilities(sentence, mask_words_list, scalar_values):\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","    model = modify_attention_heads(model, scalar_values)\n","\n","    inputs = tokenizer(sentence, return_tensors=\"pt\")\n","    mask_token_index = torch.where(inputs[\"input_ids\"][0] == tokenizer.mask_token_id)[0]\n","\n","    if len(mask_token_index) != 1:\n","        raise ValueError(\"Please provide a sentence with exactly one [MASK] token.\")\n","\n","    mask_token_index = mask_token_index.item()\n","\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    logits = outputs.logits\n","    mask_token_logits = logits[0, mask_token_index, :]\n","\n","    probabilities = torch.softmax(mask_token_logits, dim=0)\n","    word_probabilities = {word: probabilities[tokenizer.convert_tokens_to_ids(word)].item() for word in mask_words_list}\n","\n","    return word_probabilities\n","\n","def apply_bert(row, scalar_values):\n","    sentence = row['masked_sentence']\n","    mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","    word_probabilities = get_masked_token_probabilities(sentence, mask_words_list, scalar_values)\n","\n","    row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","    row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","    row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","    row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","    return row['bias_percent']\n","\n","def objective(**scalar_values):\n","    scalar_values = [scalar_values[f'scalar_{i}'] for i in range(12)]\n","    bias_percent = df_winogender.apply(apply_bert, axis=1, scalar_values=scalar_values).mean()\n","    return bias_percent\n","\n","bounds = {'scalar_' + str(i): (0.01, 10.0) for i in range(12)}\n","\n","optimizer = BayesianOptimization(\n","    f=objective,\n","    pbounds=bounds,\n","    verbose=2,\n","    random_state=1,\n",")\n","\n","optimizer.maximize(init_points=10, n_iter=50)\n","\n","optimal_scalar_values = [optimizer.max['params'][f'scalar_{i}'] for i in range(12)]\n","min_average_bias = optimizer.max['target']\n","\n","print(f\"Optimal scalar values: {optimal_scalar_values}\")\n","print(f\"Minimum average gender bias in Winogender dataset: {min_average_bias}\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"E6_8bbv6yoOy","colab":{"base_uri":"https://localhost:8080/","height":509,"referenced_widgets":["7f9d5dc10bee42d9b77821ce53dd1457","2360c633053840958e860b801a7be42a","356dc78ccc874cb4a87fc099ff576e05","903f377218024157b376b1034c8c11a1","8b948fafed0248fe84ab73a802845246","cf179965d47542bdb53024c9cd2ec29b","3ebecdd8caae4e8bb4c77c9d597a6a4f","817be466a39548029bc549105c09d9e1","aab9ec65415f4d44a4530f0c56738fd8","f8eae9e1dcf34ba2806123483f3b69b4","81c45f0f56864918bce0cd0f4ab23865","08b154b6004e42319b2c48640c405023","8be43b594b34464c8b8b73db01fee5a6","ad132624619347bcad4eaf7328ea29a9","9c1c00b79a44420594547e203ea4b544","96c6b30ab72a4bdcaecaf8cac5f603be","19c7d6a782f440db85fceebacad883fc","c1582386c6894fe28e3a3370df6c1100","65a153ef7bdf4ee5863966b64deb6a28","4ca80ef4ff90402eb3a23cf0e00ceebe","c9f069ba51e64a9d883f32a5d9f4b7d7","e7dacf86dc244cde924c406b803cf243","df20f20a75ff41e88391e0e0e39a89ed","8fb5ce1c11224818a79905bfd174907b","559006a00e60471aa9ff5c2bf4ea2c58","29d8186f5984406a9fa5c9c30b13e8a5","4b22742d46b7473a9acd4a9a550dcada","ae1dabc04e6844c1a06fd24a51abe8d4","b893705e49fa45a68cc5e33ae659df17","837f8d9cb33640efab3c403a45238fc1","89ab637e2ac84fe189f6cfda413f839b","bae6b434bda545eb9c5dbfc1ac2b5656","de7495bd1b114e56ae4286871ae41241","39750359d258490ba6e10734b0a5d594","84f3c48ebb894e5f8f1dfa7b88e4388e","ff9f35c6bec343ef82ed069fd351df76","3c48dbef814249bdb806a031b25075fe","3b0a4ba18c7d40c5a7adcf3efa5af410","3238ca54685c4d0ca78c1d3711857e00","61dea0383db94b699b152d531b7455a7","6eb5d01e6f08440b80cc5fcb9c25867e","b50e3bf247884a0abf3d87b0ae7316db","6817d068d16b4268a6f521631035d0f4","3674d49c69d04ca98df58a4332cc408a","0925814998284719bec66d6376abd311","cf8ca3405c004533bbcd2807e4e87be3","ad5448669f984cdd9b4920816889d172","38f8c206b1c2406ba5006ec036c1928c","952a484019fb4f3ebf66157e58d1438a","751e2aa84ef643ad9bac843477cd7bbb","0b53ffaaaa9a4d33856c8bd53c2969c3","ecff48c8d7d74dc3899f51bc297fa3c9","9653901fe9584de594b0dddac37857b5","da5f59ccc5134e78878f8e0a363a6319","ab2d312a7b224ddbb16eeca10fedeaaf"]},"executionInfo":{"status":"ok","timestamp":1713981738523,"user_tz":300,"elapsed":361842,"user":{"displayName":"Shweta","userId":"05242245977803223502"}},"outputId":"6b385baa-85a9-4769-ac74-38909a8767bb"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9d5dc10bee42d9b77821ce53dd1457"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08b154b6004e42319b2c48640c405023"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df20f20a75ff41e88391e0e0e39a89ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39750359d258490ba6e10734b0a5d594"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0925814998284719bec66d6376abd311"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Probabilities of specified words:\n","he: 0.41993263363838196\n","she: 0.038190655410289764\n","Mounted at /content/drive\n","-----------------------------------------------\n","Beta = [8.51, 6.92, 5.14, 0.01, 8.80, 0.01, 6.74, 0.01, 10.0, 10.0, 0.01, 6.72]\n","Winogender - Average gender bias in bert:  0.7\n","-----------------------------------------------\n","Score 0 : No bias\n","Score 1 : Complete bias towards one gender\n","-----------------------------------------------\n"]}],"source":["\n","\n","# Example sentence\n","sentence = \"The engineer informed the client that [MASK] would need more time to complete the project.\"\n","\n","# List of words to check probabilities for\n","mask_words_list = [\"he\", \"she\"]\n","\n","# Scalar values for modifying attention heads\n","scalar_values = [8.51, 6.92, 5.14, 0.01, 8.80, 0.01, 6.74, 0.01, 10.0, 10.0, 0.01, 6.72]\n","\n","\n","# Get the probabilities of specified words\n","word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","print(\"Probabilities of specified words:\")\n","for word, probability in word_probabilities.items():\n","    print(f\"{word}: {probability}\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","\n","# Read the CSV file into a pandas DataFrame\n","\n","df_winogender = pd.read_csv(\"/content/drive/MyDrive/LLM Bias Project/Winogender_dataset.csv\")\n","\n","new_cols = ['male_prob_abs','female_prob_abs', 'male_prob_percent','female_prob_percent', 'bias_percent']\n","for col_name in new_cols:\n","    df_winogender[col_name] = None\n","\n","def apply_bert(row):\n","  sentence = row['masked_sentence']\n","  mask_words_list = [row['male_pronoun'], row['female_pronoun']]\n","\n","  word_probabilities = get_masked_token_probabilities(sentence, mask_words_list)\n","\n","  row['male_prob_abs'], row['female_prob_abs'] = round(word_probabilities[row['male_pronoun']],2) , round(word_probabilities[row['female_pronoun']],2)\n","  row['male_prob_percent'] = round(word_probabilities[row['male_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['female_prob_percent'] = round(word_probabilities[row['female_pronoun']] / (word_probabilities[row['male_pronoun']] + word_probabilities[row['female_pronoun']]),2)\n","  row['bias_percent'] = round(abs(row['male_prob_percent'] - row['female_prob_percent']),2)\n","  return row\n","\n","\n","# Apply the function to each row\n","df_winogender = df_winogender.apply(apply_bert, axis=1)\n","\n","average_bias_winogender = round(df_winogender['bias_percent'].mean(),2)\n","print('-----------------------------------------------')\n","print('Beta = [8.51, 6.92, 5.14, 0.01, 8.80, 0.01, 6.74, 0.01, 10.0, 10.0, 0.01, 6.72]')\n","print('Winogender - Average gender bias in bert: ', average_bias_winogender)\n","print('-----------------------------------------------')\n","print('Score 0 : No bias')\n","print('Score 1 : Complete bias towards one gender')\n","print('-----------------------------------------------')"]},{"cell_type":"code","source":[],"metadata":{"id":"vXsBxL8-QEFS"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZQSORa/9KkTna0QQ5poI8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00e197bd2f2a47e58cd958c8184d9205":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03d18487da9a45cb9da6c3e5da29c82d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0476e8fb33014ef6a327a3c8167cba7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bd047741ce8402ca13666c65c101e55","placeholder":"","style":"IPY_MODEL_bbd72edfd1bd4d01a158b46a75adf17e","value":"tokenizer.json:100%"}},"06be9fdd2dcb4883829eac90d2f7d14d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d3c794381154e0a9000b28d4f1726aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ead6ddc6b554ec19b3d5aa1a7d1fe53":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f603b52c8a341e29e99570bf2893715":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10003730393a4804a793653ac82bda3e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"114bae8eefeb42c289963d5954a38641":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14958c7b84a445cc8beedd2a9374f7ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_60dea3cfedc34a7889c3a5e70456935b","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be07331c07004d0197c75703f9a5d94e","value":466062}},"14b06d8a3ff742b398bdb4177378ba97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"174eb98ea8dd483894a3d71e1bb2cbad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"178947d93e9145c8a16493818e9949cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"189c0e7825b44b85ae64cbea9e870e2e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ba9894acff342b3aa31311fffcde308":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c1c4a1fb4724ece8d2c9525cdf1053a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20b1c6ecd987447697a48c25d63924b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2172ce02f14a437f8e2e69fbb89f3802":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_536dca3fb93c4fc2b49a1aec5989bea0","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37d95145fa4746f1a770a37f6dee90cb","value":570}},"23c71562dc8447b59713c3f54986b362":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a2e83b07e8d44d081c22333c6b864d8","IPY_MODEL_29a99136518541668bc8291010ed27f6","IPY_MODEL_d84a2793f7db4c16901e8004fac61624"],"layout":"IPY_MODEL_20b1c6ecd987447697a48c25d63924b1"}},"2983367c06c24b40bd3b8205878c02d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29a99136518541668bc8291010ed27f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_401199fc61a54cb596f547b7bb730a88","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57dc9627371c46eeab696b945eab7613","value":48}},"2cf5673954b34fe6a17fb0312eb603cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dfe41791e464eebab8b628318511fe6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6c7c1dd45eb4bf79134a7657d009046","placeholder":"","style":"IPY_MODEL_c0895aa8acb949b3a4b28aba49429007","value":"config.json:100%"}},"3026f29344894c8b8447a2f443142ee1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3194ece35dd942d187f2e4fc4d690119":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"350d1971596341229c34af4d132025ae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37d95145fa4746f1a770a37f6dee90cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38b2adb5ab4842abaf8a23d5fab0b0c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2983367c06c24b40bd3b8205878c02d7","placeholder":"","style":"IPY_MODEL_b6112557e483492da42ba2eba4bff811","value":"570/570[00:00&lt;00:00,5.05kB/s]"}},"3a840bd0abd244e58f09cc07b96fdcfd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f02038a912a247219e9789a4a89e1314","placeholder":"","style":"IPY_MODEL_00e197bd2f2a47e58cd958c8184d9205","value":"48.0/48.0[00:00&lt;00:00,866B/s]"}},"3b9ae6c5d9704c6aa74dda821b5461f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c5c4526a36c4946898e5d1d0c114ff0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cbb658ce9c7b490abe1a05c507aad8c8","IPY_MODEL_e0b9694fe2064b1d9875bb4c3ed663f5","IPY_MODEL_3a840bd0abd244e58f09cc07b96fdcfd"],"layout":"IPY_MODEL_617bb296b2424d47a0ea04f9324ba407"}},"401199fc61a54cb596f547b7bb730a88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"439e549563274697babd05c4289defaa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"494ea026ce51485980293fcda0c6b71b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a2e83b07e8d44d081c22333c6b864d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60d56824011b4bd981ddb5f040715f26","placeholder":"","style":"IPY_MODEL_eaaee47cd66e4a38a094f3237cfed3be","value":"tokenizer_config.json:100%"}},"4ca216aa70ed4dabb0721d29eb30914a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d792e1ec84e450788e529265b24dc08":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"502121ebacec4bb2ba5a13d2036605ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_818cb38d70034f939d5465d08eb3e0bd","placeholder":"","style":"IPY_MODEL_14b06d8a3ff742b398bdb4177378ba97","value":"440M/440M[00:05&lt;00:00,113MB/s]"}},"529b1676ef994f51a7c5060bf5194a55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52d33ef4ecc04095b4656f5dacf1c008":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52f700b623bb482e8b83bbb1d9c30abe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cf5673954b34fe6a17fb0312eb603cb","placeholder":"","style":"IPY_MODEL_613984f4c3b649779627df8116b44497","value":"model.safetensors:100%"}},"536dca3fb93c4fc2b49a1aec5989bea0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5639161f9aeb4d74bef54a7ac4dde7dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dfea58ffa897480483dcc59f16d9babe","IPY_MODEL_c61cb545b55a4429bf0a5cd6546f621f","IPY_MODEL_7432eb59df3e40ca9c13544b85309ad6"],"layout":"IPY_MODEL_189c0e7825b44b85ae64cbea9e870e2e"}},"57dc9627371c46eeab696b945eab7613":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"59618e8dc2684a89a7f8eb4ce95d0233":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b5b789248cc4f2d86bbb034e1f261d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3026f29344894c8b8447a2f443142ee1","placeholder":"","style":"IPY_MODEL_06be9fdd2dcb4883829eac90d2f7d14d","value":"570/570[00:00&lt;00:00,31.0kB/s]"}},"5bd047741ce8402ca13666c65c101e55":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d5c44b7335340f4bedf847d869db43e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dd1b51f898e4ebebd981249b5d67e53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0476e8fb33014ef6a327a3c8167cba7d","IPY_MODEL_14958c7b84a445cc8beedd2a9374f7ea","IPY_MODEL_eaaffa88b4b34821be6372bc3d1beb79"],"layout":"IPY_MODEL_677192ca7ed7491e8e143d25145aa2c7"}},"5f298a762ecb4048ba2efac77df139c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f3742d864a34e7ea18dcf9ad1cfc8cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7fd127e8d1244a92878a1dab811dacf1","IPY_MODEL_7a664ecc69794d08b3fb79c414ae9561","IPY_MODEL_502121ebacec4bb2ba5a13d2036605ef"],"layout":"IPY_MODEL_0ead6ddc6b554ec19b3d5aa1a7d1fe53"}},"60d56824011b4bd981ddb5f040715f26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60dea3cfedc34a7889c3a5e70456935b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"613984f4c3b649779627df8116b44497":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"617bb296b2424d47a0ea04f9324ba407":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"677192ca7ed7491e8e143d25145aa2c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cdbcf3fc7d9468196bf4e436f2f1bb7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6df6001ccc814a3685c60868697d1a36":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7432eb59df3e40ca9c13544b85309ad6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d792e1ec84e450788e529265b24dc08","placeholder":"","style":"IPY_MODEL_bdddb1851cc846b0a9c12e8ac85148eb","value":"232k/232k[00:00&lt;00:00,3.15MB/s]"}},"752999873847421c9e4db630bab1827d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77156009fc3342c5bf4acc43b20f9356":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8081fe6be0e244abb722b3d3a5e43831","IPY_MODEL_7d902539f111494ebe817935650f0bce","IPY_MODEL_b053cadcf6dd41039ac86324ccd29d86"],"layout":"IPY_MODEL_3b9ae6c5d9704c6aa74dda821b5461f7"}},"785f9b9d35e149d69d9d72bb01c8bdba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3e56e28296947b6b565abead3e012b9","placeholder":"","style":"IPY_MODEL_9f585d6d889044b5ae7729a7d35b6cf2","value":"440M/440M[00:04&lt;00:00,84.1MB/s]"}},"7a664ecc69794d08b3fb79c414ae9561":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52d33ef4ecc04095b4656f5dacf1c008","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_59618e8dc2684a89a7f8eb4ce95d0233","value":440449768}},"7d902539f111494ebe817935650f0bce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c1c4a1fb4724ece8d2c9525cdf1053a","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9495d6620c2e474fb98d42e4e8aaebfe","value":231508}},"7e1977c1c4c54e07a1d2deaae279b088":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_92d923062e34424b9c88d81082eb6c02","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8edb94743b5d4c4ba99e89e18841b086","value":570}},"7f9face092864cddaae69f20822f4003":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7fd127e8d1244a92878a1dab811dacf1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97931a7feb5a4df5acf5010ac6e278c1","placeholder":"","style":"IPY_MODEL_439e549563274697babd05c4289defaa","value":"model.safetensors:100%"}},"8081fe6be0e244abb722b3d3a5e43831":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebc83931de3247fea8e70f591dc55167","placeholder":"","style":"IPY_MODEL_d4886e5dbd834e3faa34235cfc81ac0f","value":"vocab.txt:100%"}},"818cb38d70034f939d5465d08eb3e0bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"822ddadb6a2b458b907f574ac9ebf181":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82b3ad8f86204595a94233ee586a579d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82be3d8ca6734da7b1fd1ffd13be17b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfe6196a4121463db87677c53a31323e","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_faaadcd65cb243c697ebc669bf172853","value":440449768}},"82d690f61d5347c0acec7e20b38fff13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f603b52c8a341e29e99570bf2893715","placeholder":"","style":"IPY_MODEL_494ea026ce51485980293fcda0c6b71b","value":"466k/466k[00:00&lt;00:00,10.5MB/s]"}},"8b0a9ad2962346aab16ad3d81bfe0c00":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c8aa529fb344ff3810548433034bd59":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f162469cda254e59ac1333b22103f64e","IPY_MODEL_7e1977c1c4c54e07a1d2deaae279b088","IPY_MODEL_5b5b789248cc4f2d86bbb034e1f261d1"],"layout":"IPY_MODEL_174eb98ea8dd483894a3d71e1bb2cbad"}},"8edb94743b5d4c4ba99e89e18841b086":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91dc928336514813a5b0240acfff6450":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92d923062e34424b9c88d81082eb6c02":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9495d6620c2e474fb98d42e4e8aaebfe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"97931a7feb5a4df5acf5010ac6e278c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f585d6d889044b5ae7729a7d35b6cf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a10cf04f4eb14ef9aa8ef597bd55718f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52f700b623bb482e8b83bbb1d9c30abe","IPY_MODEL_82be3d8ca6734da7b1fd1ffd13be17b7","IPY_MODEL_caae60c12ada4e32a223d5f9257899bd"],"layout":"IPY_MODEL_bec84dd73ff049eba5267ff97f070fa4"}},"a11f133bfe19446a924e2d835fd9c9ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d853c79e8d4649bfbf5a888f6a8f9968","IPY_MODEL_f3da463594cb4d4eaccbc0dc6b8cd3ba","IPY_MODEL_785f9b9d35e149d69d9d72bb01c8bdba"],"layout":"IPY_MODEL_178947d93e9145c8a16493818e9949cd"}},"a7bf06895e8f48f2a6fa85fd93688640":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab90d937469b41fbb6149996262a7f84":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b040a46c1e804835b429f8219cf72ffb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b053cadcf6dd41039ac86324ccd29d86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bda0febee4674b16880acde0ad7935b5","placeholder":"","style":"IPY_MODEL_114bae8eefeb42c289963d5954a38641","value":"232k/232k[00:00&lt;00:00,1.90MB/s]"}},"b4eef3ee5e6c4089a06b6dcb749185e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5713f476853466490077551f03dea22":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5fdc2aff4264e1c849c45ca70a922e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2dfe41791e464eebab8b628318511fe6","IPY_MODEL_2172ce02f14a437f8e2e69fbb89f3802","IPY_MODEL_38b2adb5ab4842abaf8a23d5fab0b0c2"],"layout":"IPY_MODEL_03d18487da9a45cb9da6c3e5da29c82d"}},"b6112557e483492da42ba2eba4bff811":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbd72edfd1bd4d01a158b46a75adf17e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bda0febee4674b16880acde0ad7935b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdddb1851cc846b0a9c12e8ac85148eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be07331c07004d0197c75703f9a5d94e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bec84dd73ff049eba5267ff97f070fa4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0895aa8acb949b3a4b28aba49429007":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c28f0d78d18044d09e110398e559b273":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e13d5efcb69b4dd6b42d5d66385320df","IPY_MODEL_ecde37c7ec7b47cdae6884b888095e00","IPY_MODEL_82d690f61d5347c0acec7e20b38fff13"],"layout":"IPY_MODEL_b040a46c1e804835b429f8219cf72ffb"}},"c61cb545b55a4429bf0a5cd6546f621f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b0a9ad2962346aab16ad3d81bfe0c00","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ca216aa70ed4dabb0721d29eb30914a","value":231508}},"c6c7c1dd45eb4bf79134a7657d009046":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caae60c12ada4e32a223d5f9257899bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f109bb6b07d340d8884140a4fe608595","placeholder":"","style":"IPY_MODEL_b4eef3ee5e6c4089a06b6dcb749185e7","value":"440M/440M[00:04&lt;00:00,105MB/s]"}},"cbb658ce9c7b490abe1a05c507aad8c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_752999873847421c9e4db630bab1827d","placeholder":"","style":"IPY_MODEL_a7bf06895e8f48f2a6fa85fd93688640","value":"tokenizer_config.json:100%"}},"d3e56e28296947b6b565abead3e012b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4886e5dbd834e3faa34235cfc81ac0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d84a2793f7db4c16901e8004fac61624":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5713f476853466490077551f03dea22","placeholder":"","style":"IPY_MODEL_5f298a762ecb4048ba2efac77df139c7","value":"48.0/48.0[00:00&lt;00:00,2.79kB/s]"}},"d853c79e8d4649bfbf5a888f6a8f9968":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_350d1971596341229c34af4d132025ae","placeholder":"","style":"IPY_MODEL_ab90d937469b41fbb6149996262a7f84","value":"model.safetensors:100%"}},"dfe6196a4121463db87677c53a31323e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfea58ffa897480483dcc59f16d9babe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d5c44b7335340f4bedf847d869db43e","placeholder":"","style":"IPY_MODEL_82b3ad8f86204595a94233ee586a579d","value":"vocab.txt:100%"}},"e0b9694fe2064b1d9875bb4c3ed663f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10003730393a4804a793653ac82bda3e","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_529b1676ef994f51a7c5060bf5194a55","value":48}},"e13d5efcb69b4dd6b42d5d66385320df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3194ece35dd942d187f2e4fc4d690119","placeholder":"","style":"IPY_MODEL_6cdbcf3fc7d9468196bf4e436f2f1bb7","value":"tokenizer.json:100%"}},"ea3f8a64253a4bdf8f5f40f26f351404":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eaaee47cd66e4a38a094f3237cfed3be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eaaffa88b4b34821be6372bc3d1beb79":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_822ddadb6a2b458b907f574ac9ebf181","placeholder":"","style":"IPY_MODEL_ea3f8a64253a4bdf8f5f40f26f351404","value":"466k/466k[00:00&lt;00:00,4.73MB/s]"}},"ebc83931de3247fea8e70f591dc55167":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecde37c7ec7b47cdae6884b888095e00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ba9894acff342b3aa31311fffcde308","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f9face092864cddaae69f20822f4003","value":466062}},"f02038a912a247219e9789a4a89e1314":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f109bb6b07d340d8884140a4fe608595":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f162469cda254e59ac1333b22103f64e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91dc928336514813a5b0240acfff6450","placeholder":"","style":"IPY_MODEL_0d3c794381154e0a9000b28d4f1726aa","value":"config.json:100%"}},"f3da463594cb4d4eaccbc0dc6b8cd3ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6df6001ccc814a3685c60868697d1a36","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f60febbdc302457f8ef371b9c8a166d8","value":440449768}},"f60febbdc302457f8ef371b9c8a166d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"faaadcd65cb243c697ebc669bf172853":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f9d5dc10bee42d9b77821ce53dd1457":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2360c633053840958e860b801a7be42a","IPY_MODEL_356dc78ccc874cb4a87fc099ff576e05","IPY_MODEL_903f377218024157b376b1034c8c11a1"],"layout":"IPY_MODEL_8b948fafed0248fe84ab73a802845246"}},"2360c633053840958e860b801a7be42a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf179965d47542bdb53024c9cd2ec29b","placeholder":"","style":"IPY_MODEL_3ebecdd8caae4e8bb4c77c9d597a6a4f","value":"tokenizer_config.json:100%"}},"356dc78ccc874cb4a87fc099ff576e05":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_817be466a39548029bc549105c09d9e1","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aab9ec65415f4d44a4530f0c56738fd8","value":48}},"903f377218024157b376b1034c8c11a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8eae9e1dcf34ba2806123483f3b69b4","placeholder":"","style":"IPY_MODEL_81c45f0f56864918bce0cd0f4ab23865","value":"48.0/48.0[00:00&lt;00:00,737B/s]"}},"8b948fafed0248fe84ab73a802845246":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf179965d47542bdb53024c9cd2ec29b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ebecdd8caae4e8bb4c77c9d597a6a4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"817be466a39548029bc549105c09d9e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aab9ec65415f4d44a4530f0c56738fd8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8eae9e1dcf34ba2806123483f3b69b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81c45f0f56864918bce0cd0f4ab23865":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08b154b6004e42319b2c48640c405023":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8be43b594b34464c8b8b73db01fee5a6","IPY_MODEL_ad132624619347bcad4eaf7328ea29a9","IPY_MODEL_9c1c00b79a44420594547e203ea4b544"],"layout":"IPY_MODEL_96c6b30ab72a4bdcaecaf8cac5f603be"}},"8be43b594b34464c8b8b73db01fee5a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19c7d6a782f440db85fceebacad883fc","placeholder":"","style":"IPY_MODEL_c1582386c6894fe28e3a3370df6c1100","value":"vocab.txt:100%"}},"ad132624619347bcad4eaf7328ea29a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65a153ef7bdf4ee5863966b64deb6a28","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ca80ef4ff90402eb3a23cf0e00ceebe","value":231508}},"9c1c00b79a44420594547e203ea4b544":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9f069ba51e64a9d883f32a5d9f4b7d7","placeholder":"","style":"IPY_MODEL_e7dacf86dc244cde924c406b803cf243","value":"232k/232k[00:00&lt;00:00,1.71MB/s]"}},"96c6b30ab72a4bdcaecaf8cac5f603be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19c7d6a782f440db85fceebacad883fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1582386c6894fe28e3a3370df6c1100":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65a153ef7bdf4ee5863966b64deb6a28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ca80ef4ff90402eb3a23cf0e00ceebe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9f069ba51e64a9d883f32a5d9f4b7d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7dacf86dc244cde924c406b803cf243":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df20f20a75ff41e88391e0e0e39a89ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8fb5ce1c11224818a79905bfd174907b","IPY_MODEL_559006a00e60471aa9ff5c2bf4ea2c58","IPY_MODEL_29d8186f5984406a9fa5c9c30b13e8a5"],"layout":"IPY_MODEL_4b22742d46b7473a9acd4a9a550dcada"}},"8fb5ce1c11224818a79905bfd174907b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae1dabc04e6844c1a06fd24a51abe8d4","placeholder":"","style":"IPY_MODEL_b893705e49fa45a68cc5e33ae659df17","value":"tokenizer.json:100%"}},"559006a00e60471aa9ff5c2bf4ea2c58":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_837f8d9cb33640efab3c403a45238fc1","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89ab637e2ac84fe189f6cfda413f839b","value":466062}},"29d8186f5984406a9fa5c9c30b13e8a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bae6b434bda545eb9c5dbfc1ac2b5656","placeholder":"","style":"IPY_MODEL_de7495bd1b114e56ae4286871ae41241","value":"466k/466k[00:00&lt;00:00,2.40MB/s]"}},"4b22742d46b7473a9acd4a9a550dcada":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae1dabc04e6844c1a06fd24a51abe8d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b893705e49fa45a68cc5e33ae659df17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"837f8d9cb33640efab3c403a45238fc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89ab637e2ac84fe189f6cfda413f839b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bae6b434bda545eb9c5dbfc1ac2b5656":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de7495bd1b114e56ae4286871ae41241":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39750359d258490ba6e10734b0a5d594":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84f3c48ebb894e5f8f1dfa7b88e4388e","IPY_MODEL_ff9f35c6bec343ef82ed069fd351df76","IPY_MODEL_3c48dbef814249bdb806a031b25075fe"],"layout":"IPY_MODEL_3b0a4ba18c7d40c5a7adcf3efa5af410"}},"84f3c48ebb894e5f8f1dfa7b88e4388e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3238ca54685c4d0ca78c1d3711857e00","placeholder":"","style":"IPY_MODEL_61dea0383db94b699b152d531b7455a7","value":"config.json:100%"}},"ff9f35c6bec343ef82ed069fd351df76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6eb5d01e6f08440b80cc5fcb9c25867e","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b50e3bf247884a0abf3d87b0ae7316db","value":570}},"3c48dbef814249bdb806a031b25075fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6817d068d16b4268a6f521631035d0f4","placeholder":"","style":"IPY_MODEL_3674d49c69d04ca98df58a4332cc408a","value":"570/570[00:00&lt;00:00,6.06kB/s]"}},"3b0a4ba18c7d40c5a7adcf3efa5af410":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3238ca54685c4d0ca78c1d3711857e00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61dea0383db94b699b152d531b7455a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6eb5d01e6f08440b80cc5fcb9c25867e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b50e3bf247884a0abf3d87b0ae7316db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6817d068d16b4268a6f521631035d0f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3674d49c69d04ca98df58a4332cc408a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0925814998284719bec66d6376abd311":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cf8ca3405c004533bbcd2807e4e87be3","IPY_MODEL_ad5448669f984cdd9b4920816889d172","IPY_MODEL_38f8c206b1c2406ba5006ec036c1928c"],"layout":"IPY_MODEL_952a484019fb4f3ebf66157e58d1438a"}},"cf8ca3405c004533bbcd2807e4e87be3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_751e2aa84ef643ad9bac843477cd7bbb","placeholder":"","style":"IPY_MODEL_0b53ffaaaa9a4d33856c8bd53c2969c3","value":"model.safetensors:100%"}},"ad5448669f984cdd9b4920816889d172":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecff48c8d7d74dc3899f51bc297fa3c9","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9653901fe9584de594b0dddac37857b5","value":440449768}},"38f8c206b1c2406ba5006ec036c1928c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da5f59ccc5134e78878f8e0a363a6319","placeholder":"","style":"IPY_MODEL_ab2d312a7b224ddbb16eeca10fedeaaf","value":"440M/440M[00:02&lt;00:00,161MB/s]"}},"952a484019fb4f3ebf66157e58d1438a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"751e2aa84ef643ad9bac843477cd7bbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b53ffaaaa9a4d33856c8bd53c2969c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecff48c8d7d74dc3899f51bc297fa3c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9653901fe9584de594b0dddac37857b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da5f59ccc5134e78878f8e0a363a6319":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab2d312a7b224ddbb16eeca10fedeaaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}